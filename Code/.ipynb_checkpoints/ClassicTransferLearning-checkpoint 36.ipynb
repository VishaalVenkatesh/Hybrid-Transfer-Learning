{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Transfer Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Team 8: Pinkies*\n",
    "Yuan Feng [yf115]\\\n",
    "Sebastián Soriano Pérez [ss1072]\\\n",
    "Vishaal Venkatesh [vv58]\\\n",
    "Abhiraj Vinnakota [agv9]\\\n",
    "Roderick Whang [rjw34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Jupyter Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all necessary packages and additional setups for this notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import packages\n",
    "'''\n",
    "# Based on code from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import (average_precision_score, roc_curve, roc_auc_score, precision_recall_curve, \n",
    "                             confusion_matrix)\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set directory parameters and additional setups\n",
    "'''\n",
    "# Set the directories for the image dataset\n",
    "# (The folder structure should include /train and /val folders with /[class1] and /[class2] images inside each)\n",
    "data_dir = '../Data/hymenoptera_data'\n",
    "\n",
    "# Additional setups\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.ion()   # interactive mode\n",
    "device  = torch.device(\"cpu\")\n",
    "palette = ['darkcyan', 'turquoise', 'deeppink', 'hotpink']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of every function needed to load the datasets, preprocess the data and extract features, define the classigication models, test the cross validated performance of the models, and plot the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    '''\n",
    "    Function to load the training and validation data from the input directory\n",
    "    '''\n",
    "    data_transforms = {\n",
    "        'train': \n",
    "        transforms.Compose(\n",
    "            [transforms.RandomResizedCrop(224),\n",
    "             transforms.RandomHorizontalFlip(),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "        ),\n",
    "        'val':\n",
    "        transforms.Compose(\n",
    "            [transforms.Resize(256),\n",
    "             transforms.CenterCrop(224),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    sets = ['train', 'val']\n",
    "    \n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x), data_transforms[x]\n",
    "        ) for x in sets\n",
    "    }\n",
    "    \n",
    "    dataloaders = {\n",
    "        x: torch.utils.data.DataLoader(\n",
    "            image_datasets[x], batch_size=4, shuffle=True, num_workers=0,\n",
    "        ) for x in sets\n",
    "    }\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in sets}\n",
    "    class_names   = image_datasets['train'].classes\n",
    "    \n",
    "    return image_datasets, dataloaders, dataset_sizes, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    '''\n",
    "    Function to train the model input with the specified parameters and number of epochs.\n",
    "    \n",
    "    Returns the best model found in any given epoch, the losses found on every epoch (for train and val)\n",
    "    '''\n",
    "    since          = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc       = 0.0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses   = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('epoch {}:'.format(epoch))\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss     = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs  = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss     = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss     += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "\n",
    "            # deep copy the model if it has the best val accuracy so far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            print('{} - loss: {:.4f}; acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        print('-' * 10)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, dataloaders, train=False):\n",
    "    '''\n",
    "    Returns model's prediction scores and actual labels.\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = 'train' if train else 'val'\n",
    "    y       = []\n",
    "    scores  = []\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloaders[dataset]):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs  = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        sm            = torch.nn.Softmax()\n",
    "        probabilities = sm(outputs) \n",
    "\n",
    "        for j in range(len(labels.tolist())):\n",
    "            y.append(labels.tolist()[j])\n",
    "            scores.append(probabilities.tolist()[j][1])\n",
    "    \n",
    "    return y, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores, legend, color):\n",
    "    '''\n",
    "    Function to plot ROC curve\n",
    "    '''\n",
    "    fpr, tpr, _   = roc_curve(labels, prediction_scores, pos_label = 1)\n",
    "    auc           = roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = legend + ' ($AUC = {:0.4f}$)'.format(auc)  \n",
    "    plt.plot(fpr, tpr, label = legend_string, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prc(labels, prediction_scores, legend, color):\n",
    "    '''\n",
    "    Function to plot PRC curve\n",
    "    '''\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, prediction_scores)\n",
    "    average_precision = average_precision_score(labels, prediction_scores)\n",
    "    legend_string = legend + ' ($AP = {:0.4f}$)'.format(average_precision)  \n",
    "    plt.plot(recall, precision, label = legend_string, color = color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Fitting and Performance on Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the validation sets for both models used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading data\n",
    "'''\n",
    "image_datasets, dataloaders, dataset_sizes, class_names = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "train - loss: 0.8815; acc: 0.4836\n",
      "val - loss: 0.7228; acc: 0.6013\n",
      "----------\n",
      "\n",
      "epoch 1:\n",
      "train - loss: 0.7533; acc: 0.5410\n",
      "val - loss: 0.5491; acc: 0.7124\n",
      "----------\n",
      "\n",
      "epoch 2:\n",
      "train - loss: 0.6750; acc: 0.6393\n",
      "val - loss: 1.2649; acc: 0.5621\n",
      "----------\n",
      "\n",
      "epoch 3:\n",
      "train - loss: 0.7214; acc: 0.5984\n",
      "val - loss: 0.5835; acc: 0.6928\n",
      "----------\n",
      "\n",
      "epoch 4:\n",
      "train - loss: 0.6710; acc: 0.6557\n",
      "val - loss: 1.0982; acc: 0.6993\n",
      "----------\n",
      "\n",
      "epoch 5:\n",
      "train - loss: 0.8607; acc: 0.5820\n",
      "val - loss: 0.7547; acc: 0.6928\n",
      "----------\n",
      "\n",
      "epoch 6:\n",
      "train - loss: 0.7224; acc: 0.5615\n",
      "val - loss: 0.8167; acc: 0.6405\n",
      "----------\n",
      "\n",
      "epoch 7:\n",
      "train - loss: 0.5894; acc: 0.6844\n",
      "val - loss: 0.7141; acc: 0.6732\n",
      "----------\n",
      "\n",
      "epoch 8:\n",
      "train - loss: 0.5930; acc: 0.7008\n",
      "val - loss: 0.6204; acc: 0.7124\n",
      "----------\n",
      "\n",
      "epoch 9:\n",
      "train - loss: 0.6156; acc: 0.6639\n",
      "val - loss: 0.6262; acc: 0.6732\n",
      "----------\n",
      "\n",
      "epoch 10:\n",
      "train - loss: 0.6006; acc: 0.6803\n",
      "val - loss: 0.6019; acc: 0.6928\n",
      "----------\n",
      "\n",
      "epoch 11:\n",
      "train - loss: 0.5699; acc: 0.6844\n",
      "val - loss: 0.6382; acc: 0.7059\n",
      "----------\n",
      "\n",
      "epoch 12:\n",
      "train - loss: 0.5834; acc: 0.6762\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c1fbced15fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m model1, train_losses1, val_losses1 = train_model(\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d09101bd6778>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model 1: CNN Trained from Scratch\n",
    "'''\n",
    "# Create model with pretrained=False, weights are initialized by the resnet18 class\n",
    "model1 = models.resnet18(pretrained=False)\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "num_ftrs1 = model1.fc.in_features\n",
    "model1.fc = nn.Linear(num_ftrs1, 2)\n",
    "\n",
    "model1     = model1.to(device)\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "# All parameters are being optimized\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler1 = lr_scheduler.StepLR(optimizer1, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training\n",
    "model1, train_losses1, val_losses1 = train_model(\n",
    "    model1, criterion1, optimizer1, exp_lr_scheduler1, num_epochs=30\n",
    ")\n",
    "\n",
    "# Get model prediction scores on validation set\n",
    "y1, scores1 = get_scores(model1, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model 2: CNN Trained with Transfer Learning\n",
    "'''\n",
    "# Create model with pretrained=True, weights are set to a resnet18 model trained on the ImageNet dataset\n",
    "model2 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False # gradients won't be computed during backpropagation\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs2 = model2.fc.in_features\n",
    "model2.fc = nn.Linear(num_ftrs2, 2)\n",
    "\n",
    "model2     = model2.to(device)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only parameters of final layer are being optimized\n",
    "optimizer2 = optim.SGD(model2.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler2 = lr_scheduler.StepLR(optimizer2, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training\n",
    "model2, train_losses2, val_losses2 = train_model(\n",
    "    model2, criterion2, optimizer2, exp_lr_scheduler2, num_epochs=30\n",
    ")\n",
    "\n",
    "# Get model prediction scores on validation set\n",
    "y2, scores2 = get_scores(model2, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting Loss vs. Epoch\n",
    "'''\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('Loss vs. Epoch')\n",
    "plt.plot(range(30), train_losses1, label='Model 1 Training Loss', color=palette[0])\n",
    "plt.plot(range(30), val_losses1, label='Model 1 Validation Loss', color=palette[1], linestyle='--')\n",
    "plt.plot(range(30), train_losses2, label='Model 2 Training Loss', color=palette[2])\n",
    "plt.plot(range(30), val_losses2, label='Model 2 Validation Loss', color=palette[3], linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting ROC Curves for Performance on Validation Set\n",
    "'''\n",
    "# Compute and plot the ROC curves\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('ROC Curves')\n",
    "plt.plot([0, 1], [0, 1], '--', color = 'gray', label = 'Chance')\n",
    "\n",
    "# Plot ROC for each model\n",
    "plot_roc(y1, scores1, legend='Model 1', color=palette[0])\n",
    "plot_roc(y2, scores2, legend='Model 2', color=palette[2])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting PRC Curves for Performance on Validation Set\n",
    "'''\n",
    "# Compute and plot the PRC curves\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('Precision-Recall Curves')\n",
    "\n",
    "# Plot ROC for each model\n",
    "plot_prc(y1, scores1, legend='Model 1', color=palette[0])\n",
    "plot_prc(y2, scores2, legend='Model 2', color=palette[2])\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing confusion matrices for both models and sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Confusion Matrix for Model 1\n",
    "'''\n",
    "pd.DataFrame(confusion_matrix(y1, list(map(lambda x: round(x, 0), scores1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Confusion Matrix for Model 2\n",
    "'''\n",
    "pd.DataFrame(confusion_matrix(y2, list(map(lambda x: round(x, 0), scores2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Function to display tensor images\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images with actual labels\n",
    "'''\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "plt.figure()\n",
    "#plt.suptitle('Sample images with actual labels')\n",
    "\n",
    "for i in range(5):\n",
    "    inputs, classes = next(iter(dataloaders['train']))\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    imshow(inputs[0], class_names[classes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images with predictions with Model 1\n",
    "'''\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "plt.figure()\n",
    "#plt.suptitle('Sample images with predicted labels')\n",
    "\n",
    "for i in range(5):\n",
    "    inputs, classes = next(iter(dataloaders['train']))\n",
    "    outputs  = model1(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    imshow(inputs[0], class_names[preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images with predictions with Model 2\n",
    "'''\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "plt.figure()\n",
    "#plt.suptitle('Sample images with predicted labels')\n",
    "\n",
    "for i in range(5):\n",
    "    inputs, classes = next(iter(dataloaders['train']))\n",
    "    outputs  = model2(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    imshow(inputs[0], class_names[preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
