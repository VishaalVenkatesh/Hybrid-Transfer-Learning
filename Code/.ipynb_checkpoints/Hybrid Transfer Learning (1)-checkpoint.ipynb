{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Vishaal\\\\Documents\\\\GitHub\\\\Hybrid-Transfer-Learning\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Generic Module Imports \n",
    "\n",
    "'''\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Pennylane is an open source quantum machine learning package by Xanadu. Pennylane also allows to use IBMs Qiskit as an\n",
    "    installed plugin. We may or may not evntually use that.\n",
    "    \n",
    "    Citation: \n",
    "    Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Carsten Blank, Keri McKiernan and Nathan Killoran. \n",
    "    PennyLane. arXiv, 2018. arXiv:1811.04968\n",
    "'''\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np # Same as standard numpy, but also does automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    PyTorch is an open-source machine learning library by Facebook. It is similar to Tensorflow by Google. It's pretty \n",
    "    cool to use this to build models and import pre-trained neural nets like ResNet18. \n",
    "    \n",
    "    PyTorch uses torch tensors and not numpy arrays. We may have to convert as and when necesary. \n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn                    #This serves as a base class for neural networks (NNs)#\n",
    "import torch.optim as optim              #Contains built-in optimizer functions - like Stochastic Gradient Descent(SGD) & Adam Optimizer'''\n",
    "from torch.optim import lr_scheduler     #Helps modulate learning rate based on number of epochs\n",
    "import torchvision                       #PyTorchs imgage datasets and other related stuff\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    Initialize Some Parameters - For now we will use an integrated Pennylane Device. This is sort of a simulator for \n",
    "    a quantum computer. I will later attempt to use a real Quantum Computer using IBMs Q Experience but no guarantees. \n",
    "    This should not technically affect our results. Because we only use 4 qubits, the quantum simulator should have no \n",
    "    issues accurately simulating the process in polyomial time. Problems will arise for ~>50 qubits.\n",
    "    \n",
    "'''\n",
    "n_qubits = 4              # Number of qubits\n",
    "step = 0.0004               # Learning rate\n",
    "batch_size = 4             # Number of samples for each training step. This is the number of features taken in from ResNet18.\n",
    "num_epochs = 30            # Number of training epochs. Set to 1 to train quickly. We will later change this to 30\n",
    "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "rng_seed = 0                # Seed for random number generator\n",
    "start_time = time.time()    # Start of the computation timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Initialize the quantum device. Two options for names are Default & Gaussian. Shots is the number of times the circuit \n",
    "    will be run. 1024 or 2^10 is generally a good number. Wires is the number of modes to intialise the qubits in. \n",
    "    Remember qubits have a probabilistic nature and can take up various states. \n",
    "'''\n",
    "dev = qml.device(name = 'default.qubit', shots = 1024, wires = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Cuda is a parallel computing architecture created by Nvidia. The following line of code is to check if you have a GPU \n",
    "    in your computer and use it. Else, the CPU will be used.\n",
    "'''\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Directory for image data. Note the dataset is relatively small with only 250 images. We cannot train a whole classical\n",
    "    or quantum CNN using such small data. However, because we are using transfer learning this would suffice to do some\n",
    "    additional training. \n",
    "'''\n",
    "data_dir = \"..\\\\Data\\\\hymenoptera_data\\\\hymenoptera_data\"\n",
    "\n",
    "'''\n",
    "    Create a dictionary of transforms we need to do to the image data. For both the training and validation set.\n",
    "    Note: the values for normalise are the mean and std of the images in ImageNet (256 X 256 X 3).  \n",
    "'''\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Establish the training and validation image datasets. This is the torchvision method of loading data.\n",
    "'''\n",
    "image_datasets = {\n",
    "    x if x == \"train\" else \"validation\": datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in [\"train\", \"val\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    244 training images and 153 validation images in two different classes ants and bees\n",
    "'''\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"validation\"]}\n",
    "class_names = image_datasets[\"train\"].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Initialize the data loader. We don't load the data yet.\n",
    "'''\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size = batch_size, shuffle=True)\n",
    "    for x in [\"train\", \"validation\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Define a function to plot the images as they get ready\n",
    "'''\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    '''\n",
    "        Invert the normalisation we did before\n",
    "    '''\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    The Hadamard gate applies to a single qubit in state 0 or state 1. Once applied it results in superposition and\n",
    "    gives a 50% for state 0 or state 1 to exist. Basically - gives qubits their quantum nature.\n",
    "    https://en.wikipedia.org/wiki/Quantum_logic_gate#Hadamard_(H)_gate\n",
    "'''\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "        \n",
    "'''\n",
    "    Rotates a single qubit by theta radians along the y-axis. w is a list of angles in radians you want to rotate each\n",
    "    qubit by\n",
    "    https://www.quantum-inspire.com/kbase/ry-gate/\n",
    "    https://pennylane.readthedocs.io/en/stable/code/api/pennylane.RY.html\n",
    "'''        \n",
    "def RY_layer(w):\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "        \n",
    "'''\n",
    "    This is the controlled-not operator. The CNOT gate operates on two qubits at a time. For example, if the first qubit is\n",
    "    in state 1, the second one is flipped two. In other words, the state of the second qubit depends on the state of the \n",
    "    first one. This is kinda what quantum entanglement is. Einstein called it spooky phenomenon and although we have \n",
    "    experimental proof, we yet don't have conclusive mathematical proof from first principles to prove this. \n",
    "'''\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):  \n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  \n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "'''\n",
    "    Designing the Variational Quantum Circuit - There are three steps innvolved...\n",
    "    1. Embedding - Qubits are initialised in superimposed state. They are then rotated across to input parameters.\n",
    "    2. Variational Layers - A sequence of trainable rotation layers and constant entangling layers is applied.\n",
    "    3. Measurement - For each qubit, the expectation of the angular momentum (Z) operator is measured. This has to be done\n",
    "                     using a classical bit which can then be processed. *Most quantum phenomenon has to be measured using\n",
    "                     a classical bit. This is similar to the Schrodinger's cat experiment where quanutum phenomenon is \n",
    "                     maintined till a measurement is made. \n",
    "'''\n",
    "'''\n",
    "    When applied to a quantum function, it turns it into a QNode instance. Because, interface is set as torch, the input and \n",
    "    output have to be torch tensors. Interface can also be set to TensorFlow.\n",
    "''' \n",
    "@qml.qnode(dev, interface = 'torch')\n",
    "\n",
    "def quantum_network(q_input_features, q_weights_flat):\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    H_layer(n_qubits)\n",
    "    '''\n",
    "        Embedding\n",
    "    '''\n",
    "    RY_layer(q_input_features)\n",
    "    '''\n",
    "        Variational Layers\n",
    "    '''\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "    '''\n",
    "        Measuring Expectation - PauliZ operator applies a phase flip. PauliX is bit flip.PauliY does both.\n",
    "    '''\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    A dressed quantum circuit with classical pre-processing and post-processing layers\n",
    "'''\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(512, n_qubits)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "        self.post_net = nn.Linear(n_qubits, 2)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_network(elem, self.q_params).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating a hybrid NN using ResNet18 and the dressed quantum circuit...\n",
    "    \n",
    "'''\n",
    "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model_hybrid.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Notice that model_hybrid.fc is the last layer of ResNet18\n",
    "model_hybrid.fc = DressedQuantumNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We use cross entropy as the loss function\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We use an Adam optimizer that updates the weights of our model at each step of the process. It is a derivation\n",
    "    of Stohasti Gradient Descent (SGD) but differs in that it has a a per-parameter adaptive learning rate that depends\n",
    "    on the average of the magnitiude of recent gradiets i.e. how fast it it changing. It can provide good results fast\n",
    "    but is not always the best as SGD sometimes finds better solutions. \n",
    "'''\n",
    "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We schedule to reduce the learning rate by a factor of gamma_lr_scheduler every 10 epochs\n",
    "'''\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=7, gamma=gamma_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Training function\n",
    "'''\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0  # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    proba = []\n",
    "    print(\"Training started:\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "                # Set model to training mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to evaluate mode\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            it = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                since_batch = time.time()\n",
    "                batch_size_ = len(inputs)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track/compute gradient and make an optimization step only when training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    #_ seems to have the probabilities. preds is the actual output 0/1\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    if phase == 'validation':\n",
    "                        print(_)\n",
    "                        proba.append(_)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Print iteration results\n",
    "                running_loss += loss.item() * batch_size_\n",
    "                batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                running_corrects += batch_corrects\n",
    "                print(\n",
    "                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
    "                        phase,\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        it + 1,\n",
    "                        n_batches + 1,\n",
    "                        time.time() - since_batch,\n",
    "                    ),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                it += 1\n",
    "\n",
    "            # Print epoch results\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(\n",
    "                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
    "                    \"train\" if phase == \"train\" else \"validation  \",\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                    epoch_acc,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check if this is the best model wrt previous epochs\n",
    "            if phase == \"validation\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
    "                best_acc_train = epoch_acc\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n",
    "    return (model, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n",
      "Phase: train Epoch: 1/30 Loss: 0.6405 Acc: 0.6557        \n",
      "tensor([ 0.0869, -0.1019, -0.2760, -0.0045])\n",
      "tensor([ 0.1003,  0.3121, -0.3977, -0.6170])ch time: 4.3116\n",
      "tensor([ 0.2525, -0.2521,  0.2795, -0.5269])ch time: 4.1257\n",
      "tensor([ 0.1165, -0.1067, -0.2896, -0.2099])ch time: 4.0332\n",
      "tensor([ 3.0056e-01,  1.4087e-01,  3.1177e-01, -2.8171e-04])\n",
      "tensor([ 0.0930,  0.1191, -0.3930, -0.1924])ch time: 4.2849\n",
      "tensor([ 0.0020, -0.3533, -0.1042,  0.0337])ch time: 4.3374\n",
      "tensor([-0.3986,  0.2332, -0.0361,  0.1096])ch time: 4.0902\n",
      "tensor([-0.3274,  0.1925, -0.5684,  0.1164])ch time: 3.9280\n",
      "tensor([-0.1418,  0.1091,  0.2438,  0.1188])ch time: 4.1607\n",
      "tensor([ 0.1028,  0.2690, -0.0763, -0.4142])tch time: 4.0483\n",
      "tensor([ 0.1437, -0.1619,  0.0556, -0.1798])tch time: 4.2235\n",
      "tensor([ 0.0911, -0.0149, -0.1734, -0.5263])tch time: 4.1789\n",
      "tensor([-0.0661, -0.0314, -0.0704, -0.6077])tch time: 4.4249\n",
      "tensor([-0.0990,  0.0186,  0.0491, -0.4807])tch time: 4.2472\n",
      "tensor([ 0.2098,  0.1005,  0.0056, -0.0422])tch time: 4.1286\n",
      "tensor([ 0.3043, -0.1081,  0.0557, -0.1594])tch time: 4.3180\n",
      "tensor([-0.1230, -0.4309, -0.4395, -0.3766])tch time: 4.3032\n",
      "tensor([-0.0983, -0.0699, -0.0715,  0.0264])tch time: 3.9255\n",
      "tensor([-0.0773, -0.1338, -0.3328,  0.0584])tch time: 3.8982\n",
      "tensor([ 0.2733,  0.1304, -0.0763, -0.0872])tch time: 4.0943\n",
      "tensor([-0.1535, -0.1593, -0.2348,  0.1916])tch time: 3.6935\n",
      "tensor([-0.2996, -0.1422, -0.0916, -0.1368])tch time: 3.5525\n",
      "tensor([ 0.0097, -0.2212,  0.0119, -0.2487])tch time: 4.3562\n",
      "tensor([ 0.2231, -0.2408, -0.0544, -0.2622])tch time: 3.1062\n",
      "tensor([-0.3836, -0.4405,  0.1768, -0.1991])tch time: 3.1667\n",
      "tensor([-0.2961, -0.2753, -0.1462, -0.1115])tch time: 3.4492\n",
      "tensor([0.0128, 0.1917, 0.1956, 0.1166])9 Batch time: 3.4161\n",
      "tensor([-0.0311,  0.1165,  0.2405, -0.1380])tch time: 2.9335\n",
      "tensor([ 0.0882, -0.3552,  0.1041,  0.2186])tch time: 2.6610\n",
      "tensor([-0.2588,  0.2657,  0.2948,  0.1027])tch time: 2.6138\n",
      "tensor([-0.4388, -0.1418,  0.2161, -0.5293])tch time: 2.9577\n",
      "tensor([-0.0101,  0.2421,  0.2186, -0.2879])tch time: 3.1689\n",
      "tensor([-0.3724,  0.0306, -0.4427, -0.2261])tch time: 2.7918\n",
      "tensor([ 0.1330,  0.1322,  0.1756, -0.1436])tch time: 2.7171\n",
      "tensor([-0.1414,  0.2440,  0.0481, -0.3414])tch time: 2.8409\n",
      "tensor([-0.0377,  0.2296,  0.1412,  0.0354])tch time: 3.6559\n",
      "tensor([-0.4166, -0.0299, -0.0282,  0.0984])tch time: 2.9710\n",
      "Phase: validation   Epoch: 1/30 Loss: 0.6063 Acc: 0.6908        \n",
      "Phase: train Epoch: 2/30 Loss: 0.5491 Acc: 0.7992        \n",
      "tensor([-0.2948, -0.3155,  0.0987, -0.0021])\n",
      "tensor([0.3832, 0.3152, 0.3416, 0.2385]) Batch time: 3.7704\n",
      "tensor([ 0.4052, -0.1840,  0.4262, -0.3582])ch time: 3.7139\n",
      "tensor([0.1580, 0.4622, 0.1298, 0.0775]) Batch time: 3.6823\n",
      "tensor([ 0.3214, -0.2744,  0.1986, -0.1694])ch time: 4.1003\n",
      "tensor([-0.0959,  0.3002,  0.0779,  0.4558])ch time: 3.1235\n",
      "tensor([ 0.4483,  0.3766,  0.2595, -0.4322])ch time: 3.9716\n",
      "tensor([-0.3575,  0.1824,  0.4387,  0.3970])ch time: 3.3936\n",
      "tensor([ 0.4493, -0.1723, -0.1433,  0.1050])ch time: 2.8029\n",
      "tensor([0.2639, 0.4319, 0.4604, 0.0486]) Batch time: 4.3790\n",
      "tensor([ 0.1854, -0.2894, -0.2837,  0.2962])tch time: 4.1218\n",
      "tensor([-0.2002, -0.0747, -0.2123,  0.0776])tch time: 4.1969\n",
      "tensor([-0.3422, -0.3002,  0.3068, -0.0565])tch time: 3.3135\n",
      "tensor([-0.1661,  0.4622,  0.0177,  0.3821])tch time: 3.2168\n",
      "tensor([-0.2694, -0.3020,  0.3393, -0.2257])tch time: 4.4022\n",
      "tensor([-0.1891,  0.4563,  0.1730,  0.4675])tch time: 3.0853\n",
      "tensor([-0.0746,  0.4703,  0.4636,  0.0807])tch time: 3.2230\n",
      "tensor([ 0.1385, -0.1409, -0.2620,  0.3754])tch time: 3.5740\n",
      "tensor([-0.3690,  0.3907, -0.4476, -0.6396])tch time: 3.9538\n",
      "tensor([ 0.1838,  0.1098,  0.4663, -0.3782])tch time: 4.1796\n",
      "tensor([0.2746, 0.4476, 0.4624, 0.0988])9 Batch time: 3.7705\n",
      "tensor([-0.1193, -0.2458,  0.4244,  0.3696])tch time: 3.6702\n",
      "tensor([ 0.4702,  0.4278,  0.0914, -0.1067])tch time: 3.8974\n",
      "tensor([ 0.4564, -0.0028, -0.3622,  0.4400])tch time: 3.9885\n",
      "tensor([ 0.1995,  0.4658,  0.3271, -0.5171])tch time: 3.3079\n",
      "tensor([-0.1631, -0.3086,  0.0627,  0.0772])tch time: 3.7250\n",
      "tensor([-0.2430,  0.2865,  0.1960,  0.4672])tch time: 3.7398\n",
      "tensor([-0.1702,  0.4606, -0.0146,  0.1650])tch time: 3.9401\n",
      "tensor([-0.2675,  0.0659,  0.4685,  0.4521])tch time: 4.1073\n",
      "tensor([-0.1946,  0.4597,  0.4183, -0.0493])tch time: 3.9742\n",
      "tensor([-0.1386,  0.3777, -0.0075, -0.0339])tch time: 4.2091\n",
      "tensor([ 0.3515, -0.3038,  0.0848, -0.2784])tch time: 3.5779\n",
      "tensor([-0.0235, -0.4005,  0.4392,  0.1422])tch time: 4.1553\n",
      "tensor([0.4499, 0.4406, 0.1168, 0.0970])9 Batch time: 3.5200\n",
      "tensor([0.4661, 0.4195, 0.4441, 0.4682])9 Batch time: 3.7913\n",
      "tensor([0.4641, 0.3738, 0.2228, 0.4411])9 Batch time: 3.8490\n",
      "tensor([ 0.3907, -0.4900, -0.4467, -0.0319])tch time: 3.5833\n",
      "tensor([0.4608, 0.0966, 0.4493, 0.3330])9 Batch time: 3.7212\n",
      "Phase: validation   Epoch: 2/30 Loss: 0.5017 Acc: 0.8289        \n",
      "Phase: train Epoch: 3/30 Loss: 0.4928 Acc: 0.8279        \n",
      "tensor([-0.4011,  0.1522, -0.0271,  0.4630])\n",
      "tensor([-0.3219, -0.0134,  0.0364,  0.3280])ch time: 1.8220\n",
      "tensor([ 0.5165, -0.4729, -0.2240,  0.5230])ch time: 1.7952\n",
      "tensor([ 0.0284, -0.0187,  0.0688,  0.4020])ch time: 1.8841\n",
      "tensor([0.3345, 0.3506, 0.4951, 0.5290]) Batch time: 1.6069\n",
      "tensor([-0.0260, -0.0325,  0.0068,  0.0990])ch time: 1.6052\n",
      "tensor([ 0.4624, -0.3928,  0.1391,  0.3336])ch time: 1.6415\n",
      "tensor([ 0.4187, -0.1694, -0.3376,  0.0914])ch time: 1.6355\n",
      "tensor([ 0.0202, -0.1202,  0.1565,  0.3460])ch time: 1.9234\n",
      "tensor([ 0.3957,  0.5225,  0.5086, -0.3671])ch time: 1.2788\n",
      "tensor([ 0.1696, -0.0104,  0.4574, -0.0844])tch time: 1.2525\n",
      "tensor([ 0.2186, -0.1398, -0.0799,  0.0792])tch time: 1.2491\n",
      "tensor([-0.1135,  0.1477,  0.5259,  0.1623])tch time: 1.2499\n",
      "tensor([ 0.4537,  0.1665, -0.5489, -0.4401])tch time: 1.3081\n",
      "tensor([-0.0410, -0.6024,  0.5141,  0.5003])tch time: 1.3452\n",
      "tensor([ 0.5147, -0.5872, -0.3887, -0.4799])tch time: 1.7842\n",
      "tensor([ 0.1040, -0.0301, -0.2171, -0.0846])tch time: 1.2814\n",
      "tensor([ 0.0228, -0.1553,  0.5291,  0.0043])tch time: 1.2415\n",
      "tensor([-0.1891,  0.1572,  0.1798,  0.4861])tch time: 1.4281\n",
      "tensor([ 0.5260, -0.3345,  0.3226,  0.1695])tch time: 1.3792\n",
      "tensor([-0.0765,  0.1512,  0.1886,  0.0577])tch time: 1.3694\n",
      "tensor([ 0.3945,  0.5300, -0.2444, -0.4074])tch time: 1.4436\n",
      "tensor([ 0.1663,  0.4929, -0.1640,  0.5288])tch time: 1.2837\n",
      "tensor([ 0.1655,  0.5083, -0.4134,  0.1695])tch time: 1.3445\n",
      "tensor([ 0.5251,  0.1624,  0.5251, -0.1029])tch time: 1.3171\n",
      "tensor([0.2522, 0.1657, 0.5266, 0.4720])9 Batch time: 1.3231\n",
      "tensor([ 0.3164,  0.5034, -0.0967, -0.6161])tch time: 1.2669\n",
      "tensor([ 0.1412,  0.5274, -0.3254,  0.3294])tch time: 1.3704\n",
      "tensor([ 4.9039e-01,  3.9041e-04, -3.2299e-01, -2.4901e-01])\n",
      "tensor([ 0.1782, -0.0495, -0.4008, -0.1480])tch time: 1.5848\n",
      "tensor([-0.1075,  0.0851,  0.5266,  0.1368])tch time: 1.2782\n",
      "tensor([ 0.2447,  0.4896, -0.1606,  0.5259])tch time: 1.3653\n",
      "tensor([-0.4851,  0.1607,  0.1674,  0.0945])tch time: 1.2974\n",
      "tensor([0.4468, 0.3640, 0.1908, 0.0301])9 Batch time: 1.3484\n",
      "tensor([ 0.4864, -0.0334,  0.4309, -0.2945])tch time: 1.2789\n",
      "tensor([-0.3524,  0.1052, -0.2727,  0.5004])tch time: 1.3829\n",
      "tensor([0.3196, 0.1190, 0.4594, 0.4580])9 Batch time: 1.3744\n",
      "tensor([-0.0295,  0.5237, -0.0566,  0.1289])tch time: 1.2837\n",
      "Phase: validation   Epoch: 3/30 Loss: 0.4457 Acc: 0.9079        \n",
      "Phase: train Epoch: 4/30 Loss: 0.4647 Acc: 0.8607        \n",
      "tensor([0.5840, 0.2246, 0.5693, 0.3791])\n",
      "tensor([ 0.2132,  0.3272, -0.0070,  0.5162])ch time: 1.5198\n",
      "tensor([0.5274, 0.2145, 0.5884, 0.2077]) Batch time: 1.5261\n",
      "tensor([ 0.2272, -0.0186,  0.5048,  0.3658])ch time: 1.5548\n",
      "tensor([ 0.4901, -0.1847,  0.3267,  0.2010])ch time: 1.5407\n",
      "tensor([ 0.5376,  0.5341, -0.3616,  0.2894])ch time: 1.5370\n",
      "tensor([ 0.2300, -0.3051, -0.3333,  0.3255])ch time: 1.6749\n",
      "tensor([ 0.1758, -0.2410,  0.1840,  0.1875])ch time: 1.5904\n",
      "tensor([0.2269, 0.5901, 0.5852, 0.1147]) Batch time: 1.5803\n",
      "tensor([0.4844, 0.2087, 0.2258, 0.1908]) Batch time: 1.5218\n",
      "tensor([0.4729, 0.1534, 0.2258, 0.0239])9 Batch time: 1.5212\n",
      "tensor([0.2258, 0.5748, 0.2346, 0.4671])9 Batch time: 2.7969\n",
      "tensor([-0.0223,  0.5052,  0.1970,  0.5015])tch time: 2.1516\n",
      "tensor([-0.2089, -0.0171,  0.1782,  0.2131])tch time: 1.5671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4347,  0.5793, -0.0936,  0.1964])tch time: 1.5069\n",
      "tensor([0.5787, 0.2430, 0.5655, 0.5260])9 Batch time: 1.4984\n",
      "tensor([0.0010, 0.0705, 0.1978, 0.1948])9 Batch time: 1.4778\n",
      "tensor([0.2836, 0.1610, 0.3183, 0.5770])9 Batch time: 1.6510\n",
      "tensor([0.4349, 0.1427, 0.5843, 0.2272])9 Batch time: 2.4545\n",
      "tensor([ 0.5323,  0.2205, -0.2471, -0.2822])tch time: 1.6158\n",
      "tensor([0.5265, 0.2197, 0.5527, 0.5853])9 Batch time: 2.2166\n",
      "tensor([0.5661, 0.1318, 0.4837, 0.2080])9 Batch time: 1.6361\n",
      "tensor([0.2174, 0.0376, 0.2180, 0.5713])9 Batch time: 1.6531\n",
      "tensor([0.3026, 0.1187, 0.2023, 0.1445])9 Batch time: 1.8291\n",
      "tensor([0.2323, 0.1824, 0.5519, 0.2266])9 Batch time: 1.5872\n",
      "tensor([ 0.1883, -0.3155, -0.4952,  0.0640])tch time: 1.4834\n",
      "tensor([0.5612, 0.4684, 0.2318, 0.1718])9 Batch time: 1.5022\n",
      "tensor([0.5830, 0.5767, 0.0728, 0.1155])9 Batch time: 1.5375\n",
      "tensor([ 0.4311, -0.0484,  0.2189,  0.2275])tch time: 1.6632\n",
      "tensor([0.0716, 0.0189, 0.0068, 0.2273])9 Batch time: 2.6668\n",
      "tensor([0.3378, 0.5895, 0.2271, 0.2313])9 Batch time: 2.0165\n",
      "tensor([ 0.5747,  0.1573, -0.2853,  0.2328])tch time: 2.1879\n",
      "tensor([ 0.0817, -0.0582,  0.2337,  0.3081])tch time: 2.2911\n",
      "tensor([ 0.2521,  0.1466, -0.0521,  0.5433])tch time: 1.9598\n",
      "tensor([0.1542, 0.4975, 0.2137, 0.2661])9 Batch time: 1.9972\n",
      "tensor([-0.4618,  0.5422,  0.5764,  0.0731])tch time: 1.9716\n",
      "tensor([0.2278, 0.4974, 0.2247, 0.3578])9 Batch time: 2.0093\n",
      "tensor([0.1340, 0.1810, 0.1235, 0.4389])9 Batch time: 1.9330\n",
      "Phase: validation   Epoch: 4/30 Loss: 0.4153 Acc: 0.8816        \n",
      "Phase: train Epoch: 5/30 Loss: 0.4404 Acc: 0.8607        \n",
      "tensor([ 0.6039,  0.1893,  0.6271, -0.0816])\n",
      "tensor([ 0.5750,  0.5727,  0.2430, -0.0147])ch time: 1.5192\n",
      "tensor([0.6310, 0.0118, 0.4326, 0.2563]) Batch time: 1.5437\n",
      "tensor([ 0.6467, -0.3176,  0.2388, -0.3831])ch time: 1.8903\n",
      "tensor([0.6391, 0.3592, 0.2780, 0.6369]) Batch time: 1.7407\n",
      "tensor([0.0891, 0.2798, 0.6465, 0.2679]) Batch time: 1.6909\n",
      "tensor([ 0.6468, -0.3069,  0.6147,  0.5646])ch time: 1.5321\n",
      "tensor([ 0.2779,  0.6152,  0.2451, -0.2429])ch time: 1.5292\n",
      "tensor([ 0.6468, -0.0419,  0.5015,  0.2718])ch time: 1.6908\n",
      "tensor([-0.3175,  0.2097,  0.6423,  0.6461])ch time: 1.7594\n",
      "tensor([-0.1548,  0.5996,  0.6374,  0.1814])tch time: 1.5278\n",
      "tensor([0.6253, 0.6464, 0.2697, 0.5418])9 Batch time: 1.5476\n",
      "tensor([-0.1315,  0.6426, -0.1701,  0.2750])tch time: 1.4941\n",
      "tensor([ 0.1984,  0.0574,  0.5857, -0.4658])tch time: 1.7348\n",
      "tensor([0.1118, 0.1687, 0.6450, 0.1112])9 Batch time: 1.8029\n",
      "tensor([0.6385, 0.5309, 0.5204, 0.6456])9 Batch time: 1.6427\n",
      "tensor([ 0.6325, -0.4499,  0.6347,  0.5244])tch time: 1.8335\n",
      "tensor([0.2793, 0.6232, 0.2756, 0.2096])9 Batch time: 1.6394\n",
      "tensor([ 0.0196, -0.4117,  0.6368, -0.2903])tch time: 1.9177\n",
      "tensor([0.6051, 0.6055, 0.1645, 0.2767])9 Batch time: 2.2331\n",
      "tensor([-0.0539,  0.6059,  0.6466,  0.6467])tch time: 1.8187\n",
      "tensor([0.4113, 0.2784, 0.1395, 0.0257])9 Batch time: 1.7799\n",
      "tensor([0.6468, 0.2420, 0.2802, 0.5623])9 Batch time: 1.6395\n",
      "tensor([0.2556, 0.6354, 0.2521, 0.2104])9 Batch time: 1.7197\n",
      "tensor([ 6.4261e-01,  5.3832e-01,  1.9004e-01, -9.5099e-05])\n",
      "tensor([0.1676, 0.6343, 0.6456, 0.2768])9 Batch time: 1.5025\n",
      "tensor([ 0.6431,  0.1783,  0.6418, -0.2579])tch time: 1.5130\n",
      "tensor([ 0.0459, -0.5279,  0.0482, -0.2648])tch time: 1.4866\n",
      "tensor([0.6184, 0.2579, 0.2167, 0.0062])9 Batch time: 1.4848\n",
      "tensor([0.6469, 0.4909, 0.6313, 0.2727])9 Batch time: 1.6662\n",
      "tensor([0.6465, 0.1452, 0.1076, 0.2656])9 Batch time: 1.8641\n",
      "tensor([0.2413, 0.0596, 0.1257, 0.3278])9 Batch time: 1.5200\n",
      "tensor([-0.1895, -0.0691,  0.6463,  0.2712])tch time: 1.5728\n",
      "tensor([ 0.2480, -0.1090,  0.5305,  0.4018])tch time: 1.5219\n",
      "tensor([ 0.1653,  0.6093,  0.5050, -0.5355])tch time: 1.5011\n",
      "tensor([-0.0676,  0.0240,  0.6131,  0.1974])tch time: 1.4845\n",
      "tensor([ 0.0912,  0.4855,  0.6377, -0.1540])tch time: 1.5070\n",
      "tensor([0.2752, 0.2884, 0.2498, 0.6420])9 Batch time: 1.5902\n",
      "Phase: validation   Epoch: 5/30 Loss: 0.3631 Acc: 0.9276        \n",
      "Phase: train Epoch: 6/30 Loss: 0.4057 Acc: 0.8648        \n",
      "tensor([ 0.6181,  0.2380,  0.6952, -0.2737])\n",
      "tensor([0.3244, 0.2974, 0.6952, 0.6597]) Batch time: 1.5326\n",
      "tensor([0.6946, 0.2422, 0.2632, 0.6949]) Batch time: 1.5704\n",
      "tensor([0.1811, 0.3006, 0.0500, 0.0412]) Batch time: 1.5654\n",
      "tensor([0.3284, 0.6947, 0.2257, 0.2981]) Batch time: 1.4794\n",
      "tensor([ 0.6944, -0.1135,  0.3185,  0.0269])ch time: 1.7585\n",
      "tensor([-0.0105,  0.2610,  0.6952,  0.6897])ch time: 1.5147\n",
      "tensor([0.1370, 0.6949, 0.6913, 0.1583]) Batch time: 1.4918\n",
      "tensor([-0.3127,  0.3178,  0.3241,  0.2922])ch time: 1.5432\n",
      "tensor([0.3092, 0.0821, 0.6930, 0.0353]) Batch time: 1.5024\n",
      "tensor([0.1499, 0.2942, 0.6439, 0.1509])9 Batch time: 1.5188\n",
      "tensor([-0.1610,  0.6951,  0.3298,  0.6921])tch time: 1.4745\n",
      "tensor([0.2810, 0.6952, 0.6216, 0.5986])9 Batch time: 1.6015\n",
      "tensor([0.6951, 0.0944, 0.6739, 0.2813])9 Batch time: 1.5511\n",
      "tensor([-0.4251,  0.6117,  0.2712,  0.3296])tch time: 1.5152\n",
      "tensor([0.6863, 0.6023, 0.5908, 0.6867])9 Batch time: 1.5034\n",
      "tensor([0.6949, 0.6843, 0.6940, 0.2109])9 Batch time: 1.4769\n",
      "tensor([0.2488, 0.0916, 0.3123, 0.6892])9 Batch time: 1.5360\n",
      "tensor([0.6610, 0.6694, 0.1242, 0.1963])9 Batch time: 1.5106\n",
      "tensor([0.3096, 0.3283, 0.6132, 0.6952])9 Batch time: 1.5053\n",
      "tensor([0.0941, 0.6605, 0.4505, 0.0511])9 Batch time: 1.5409\n",
      "tensor([0.1120, 0.3111, 0.0672, 0.2443])9 Batch time: 1.5097\n",
      "tensor([ 0.6723,  0.1753, -0.2366,  0.0936])tch time: 1.4991\n",
      "tensor([ 0.3449,  0.3294, -0.2429,  0.6952])tch time: 1.4816\n",
      "tensor([ 0.6952, -0.3264,  0.3259,  0.2172])tch time: 1.4828\n",
      "tensor([0.6953, 0.1741, 0.3105, 0.0406])9 Batch time: 1.4909\n",
      "tensor([0.6156, 0.3234, 0.6926, 0.6941])9 Batch time: 1.5075\n",
      "tensor([0.5050, 0.6336, 0.3681, 0.6381])9 Batch time: 1.4870\n",
      "tensor([0.6237, 0.6940, 0.2762, 0.6571])9 Batch time: 1.5113\n",
      "tensor([0.1660, 0.6824, 0.6718, 0.6941])9 Batch time: 1.4748\n",
      "tensor([0.6780, 0.3085, 0.6881, 0.0300])9 Batch time: 1.5528\n",
      "tensor([0.6890, 0.2418, 0.6921, 0.6701])9 Batch time: 1.5273\n",
      "tensor([0.6814, 0.5024, 0.1120, 0.6952])9 Batch time: 1.5209\n",
      "tensor([0.1424, 0.3111, 0.6453, 0.6796])9 Batch time: 1.4762\n",
      "tensor([ 0.6951,  0.6949,  0.6949, -0.2839])tch time: 1.5044\n",
      "tensor([0.3120, 0.5807, 0.3271, 0.6928])9 Batch time: 1.4581\n",
      "tensor([-0.1828,  0.6904,  0.3319,  0.1121])tch time: 1.4517\n",
      "tensor([0.3029, 0.6904, 0.6817, 0.6867])9 Batch time: 1.4358\n",
      "Phase: validation   Epoch: 6/30 Loss: 0.3316 Acc: 0.9474        \n",
      "Phase: train Epoch: 7/30 Loss: 0.4066 Acc: 0.8484        \n",
      "tensor([0.0905, 0.6667, 0.2919, 0.6509])\n",
      "tensor([ 0.3240,  0.6983,  0.0379, -0.3991])ch time: 1.5061\n",
      "tensor([-0.0749,  0.1821,  0.5310,  0.3091])ch time: 1.4740\n",
      "tensor([ 0.6994, -0.1829,  0.2881,  0.2799])ch time: 1.4628\n",
      "tensor([0.6476, 0.2896, 0.3309, 0.6992]) Batch time: 1.5333\n",
      "tensor([0.2673, 0.3255, 0.6644, 0.6386]) Batch time: 1.5097\n",
      "tensor([0.6013, 0.2400, 0.1356, 0.6963]) Batch time: 1.5383\n",
      "tensor([0.3352, 0.2837, 0.3015, 0.6426]) Batch time: 1.4627\n",
      "tensor([0.2786, 0.1187, 0.2778, 0.3320]) Batch time: 1.5143\n",
      "tensor([0.3182, 0.3341, 0.6992, 0.5187]) Batch time: 1.5184\n",
      "tensor([0.3348, 0.6778, 0.6736, 0.6932])9 Batch time: 1.6694\n",
      "tensor([ 0.3247,  0.6691,  0.6776, -0.2033])tch time: 1.6540\n",
      "tensor([ 0.6672,  0.6954, -0.3762,  0.3241])tch time: 1.5169\n",
      "tensor([0.5487, 0.6996, 0.6990, 0.6292])9 Batch time: 1.5036\n",
      "tensor([ 0.3125, -0.1886,  0.6991,  0.6995])tch time: 1.4950\n",
      "tensor([0.4943, 0.6725, 0.3120, 0.3204])9 Batch time: 1.4991\n",
      "tensor([-0.2360,  0.6804,  0.5131,  0.6938])tch time: 1.4779\n",
      "tensor([0.0081, 0.2955, 0.6789, 0.6972])9 Batch time: 1.4934\n",
      "tensor([0.6887, 0.2158, 0.6983, 0.6456])9 Batch time: 1.6546\n",
      "tensor([0.6369, 0.2661, 0.1674, 0.6990])9 Batch time: 1.5639\n",
      "tensor([0.0754, 0.6545, 0.6993, 0.4274])9 Batch time: 1.5324\n",
      "tensor([0.2074, 0.0712, 0.2515, 0.6932])9 Batch time: 1.5303\n",
      "tensor([0.2308, 0.6776, 0.6976, 0.6970])9 Batch time: 1.4737\n",
      "tensor([0.2042, 0.6995, 0.6437, 0.0336])9 Batch time: 1.4901\n",
      "tensor([ 0.5574,  0.2934,  0.6899, -0.3544])tch time: 1.5111\n",
      "tensor([0.6806, 0.3348, 0.2655, 0.6914])9 Batch time: 1.5009\n",
      "tensor([0.5526, 0.1519, 0.6979, 0.2508])9 Batch time: 1.5132\n",
      "tensor([0.0891, 0.3152, 0.6995, 0.3037])9 Batch time: 1.5615\n",
      "tensor([0.2109, 0.5587, 0.6808, 0.3199])9 Batch time: 1.5616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1919,  0.2980, -0.3609,  0.3338])tch time: 1.5336\n",
      "tensor([0.6977, 0.3334, 0.6902, 0.1559])9 Batch time: 1.7422\n",
      "tensor([ 0.2978,  0.6991, -0.0418,  0.6996])tch time: 1.5027\n",
      "tensor([ 0.6989,  0.1303, -0.1079,  0.2481])tch time: 1.4630\n",
      "tensor([ 0.0759,  0.2531, -0.2030,  0.4757])tch time: 1.5136\n",
      "tensor([-0.2058,  0.3322,  0.3341,  0.3145])tch time: 1.5054\n",
      "tensor([0.6958, 0.6995, 0.4909, 0.0694])9 Batch time: 1.5428\n",
      "tensor([ 0.6479, -0.3434,  0.6980,  0.6750])tch time: 1.5113\n",
      "tensor([ 0.2711, -0.0245,  0.3058,  0.2146])tch time: 1.4720\n",
      "Phase: validation   Epoch: 7/30 Loss: 0.3260 Acc: 0.9408        \n",
      "Phase: train Epoch: 8/30 Loss: 0.3997 Acc: 0.8648        \n",
      "tensor([0.2166, 0.6965, 0.3001, 0.7025])\n",
      "tensor([0.0379, 0.7003, 0.6777, 0.2940]) Batch time: 1.5211\n",
      "tensor([0.7020, 0.3092, 0.3394, 0.3318]) Batch time: 1.5784\n",
      "tensor([0.1606, 0.2573, 0.0146, 0.2808]) Batch time: 1.5326\n",
      "tensor([0.6819, 0.6016, 0.3241, 0.6513]) Batch time: 1.5105\n",
      "tensor([0.2676, 0.4874, 0.6231, 0.7025]) Batch time: 1.5150\n",
      "tensor([0.6962, 0.3100, 0.5215, 0.6876]) Batch time: 1.5060\n",
      "tensor([0.2608, 0.4462, 0.3255, 0.3391]) Batch time: 1.4822\n",
      "tensor([-0.3159,  0.3696,  0.3380,  0.1949])ch time: 1.5290\n",
      "tensor([0.3113, 0.3389, 0.6597, 0.1681]) Batch time: 1.5359\n",
      "tensor([0.3320, 0.3327, 0.6787, 0.7030])9 Batch time: 1.4854\n",
      "tensor([5.4900e-01, 6.7994e-03, 4.0534e-04, 6.1194e-01])5219\n",
      "tensor([-0.1858,  0.6962,  0.6262,  0.7001])tch time: 1.4967\n",
      "tensor([0.2671, 0.1004, 0.2122, 0.3399])9 Batch time: 1.4994\n",
      "tensor([ 0.6399,  0.2494,  0.6743, -0.1055])tch time: 1.5457\n",
      "tensor([ 0.7035,  0.2241, -0.3285,  0.2442])tch time: 1.4883\n",
      "tensor([0.5880, 0.3298, 0.0096, 0.3208])9 Batch time: 1.5343\n",
      "tensor([ 0.3277,  0.6730,  0.3386, -0.5115])tch time: 1.4885\n",
      "tensor([0.2886, 0.3308, 0.6261, 0.0888])9 Batch time: 1.5564\n",
      "tensor([0.0624, 0.6983, 0.5882, 0.2396])9 Batch time: 1.4830\n",
      "tensor([0.6213, 0.2121, 0.6087, 0.2372])9 Batch time: 1.5425\n",
      "tensor([0.3157, 0.3335, 0.3384, 0.2386])9 Batch time: 1.5245\n",
      "tensor([0.3398, 0.7023, 0.3961, 0.6968])9 Batch time: 1.5150\n",
      "tensor([ 0.2539, -0.3314,  0.3400,  0.6056])tch time: 1.4988\n",
      "tensor([0.0799, 0.1561, 0.6861, 0.3416])9 Batch time: 1.6599\n",
      "tensor([0.4095, 0.6594, 0.2929, 0.2883])9 Batch time: 1.4629\n",
      "tensor([ 0.2325,  0.2367, -0.0314,  0.3171])tch time: 1.5169\n",
      "tensor([0.3375, 0.6527, 0.3312, 0.6355])9 Batch time: 1.5042\n",
      "tensor([ 0.5507,  0.2825,  0.5475, -0.4251])tch time: 1.5279\n",
      "tensor([ 0.6988,  0.6944, -0.2689,  0.1709])tch time: 1.5216\n",
      "tensor([0.6934, 0.3069, 0.3284, 0.3395])9 Batch time: 1.5307\n",
      "tensor([0.3343, 0.6914, 0.7033, 0.2297])9 Batch time: 1.5218\n",
      "tensor([-0.1764,  0.7039,  0.2059,  0.3291])tch time: 1.5285\n",
      "tensor([ 0.6808,  0.4995,  0.6835, -0.1971])tch time: 1.4990\n",
      "tensor([0.3184, 0.4464, 0.3092, 0.7036])9 Batch time: 1.5228\n",
      "tensor([ 0.6727, -0.2655,  0.7036,  0.5858])tch time: 1.4803\n",
      "tensor([0.6957, 0.7032, 0.2805, 0.3205])9 Batch time: 1.5015\n",
      "tensor([ 0.2990, -0.2854,  0.5885,  0.7026])tch time: 1.5278\n",
      "Phase: validation   Epoch: 8/30 Loss: 0.3441 Acc: 0.9145        \n",
      "Phase: train Epoch: 9/30 Loss: 0.3918 Acc: 0.8648        \n",
      "tensor([0.1952, 0.6776, 0.7061, 0.6547])\n",
      "tensor([0.5475, 0.7085, 0.6892, 0.4347]) Batch time: 1.5721\n",
      "tensor([0.7087, 0.0161, 0.3397, 0.0816]) Batch time: 1.5266\n",
      "tensor([ 0.3116, -0.0363,  0.7083,  0.3140])ch time: 1.5331\n",
      "tensor([0.1147, 0.6337, 0.6693, 0.6551]) Batch time: 1.5392\n",
      "tensor([0.1200, 0.3177, 0.2885, 0.7084]) Batch time: 1.5382\n",
      "tensor([0.7072, 0.7090, 0.5772, 0.3391]) Batch time: 1.5188\n",
      "tensor([ 0.1570,  0.6939,  0.1997, -0.0334])ch time: 1.4937\n",
      "tensor([0.3295, 0.6989, 0.6633, 0.6933]) Batch time: 1.5183\n",
      "tensor([0.6958, 0.6814, 0.3192, 0.6819]) Batch time: 1.5178\n",
      "tensor([-0.2108,  0.6631,  0.7030,  0.6821])tch time: 1.4907\n",
      "tensor([0.6876, 0.1582, 0.3384, 0.5763])9 Batch time: 1.5273\n",
      "tensor([0.2556, 0.2860, 0.1820, 0.2033])9 Batch time: 1.5064\n",
      "tensor([0.6076, 0.2906, 0.3007, 0.2922])9 Batch time: 1.5533\n",
      "tensor([0.6046, 0.7083, 0.7051, 0.7090])9 Batch time: 1.5707\n",
      "tensor([ 0.3443,  0.6606,  0.0119, -0.4418])tch time: 1.5423\n",
      "tensor([-0.2251, -0.3730,  0.3432,  0.0499])tch time: 1.4690\n",
      "tensor([0.1740, 0.7037, 0.3005, 0.7056])9 Batch time: 1.5189\n",
      "tensor([ 0.3188,  0.2883, -0.0285, -0.2263])tch time: 1.5066\n",
      "tensor([0.2495, 0.1668, 0.3127, 0.3326])9 Batch time: 1.6448\n",
      "tensor([ 0.7089, -0.0804,  0.7091,  0.3180])tch time: 1.4985\n",
      "tensor([ 7.0437e-01, -6.1017e-04,  3.2970e-01,  3.2329e-01])\n",
      "tensor([0.7018, 0.1813, 0.6564, 0.7089])9 Batch time: 1.5093\n",
      "tensor([ 0.3436,  0.6239,  0.6969, -0.0894])tch time: 1.5218\n",
      "tensor([0.5370, 0.7086, 0.0062, 0.2474])9 Batch time: 1.4846\n",
      "tensor([0.7083, 0.7011, 0.1417, 0.1937])9 Batch time: 1.5211\n",
      "tensor([ 0.6714, -0.0422,  0.7087,  0.7091])tch time: 1.5058\n",
      "tensor([ 0.3157,  0.3319,  0.3196, -0.2456])tch time: 1.5700\n",
      "tensor([0.7090, 0.2339, 0.1993, 0.1071])9 Batch time: 1.7136\n",
      "tensor([0.6941, 0.2588, 0.5356, 0.7087])9 Batch time: 1.5532\n",
      "tensor([0.2121, 0.7077, 0.3368, 0.5709])9 Batch time: 1.7883\n",
      "tensor([ 0.1623,  0.3438, -0.3444,  0.1636])tch time: 1.6072\n",
      "tensor([0.4317, 0.7078, 0.3409, 0.6498])9 Batch time: 1.5099\n",
      "tensor([ 0.6918,  0.1023, -0.1027,  0.3222])tch time: 1.5406\n",
      "tensor([0.7077, 0.1754, 0.6978, 0.7090])9 Batch time: 1.5385\n",
      "tensor([0.1426, 0.6746, 0.6442, 0.3426])9 Batch time: 1.5613\n",
      "tensor([ 0.6790,  0.2964, -0.2965,  0.5606])tch time: 1.5278\n",
      "tensor([ 0.3065,  0.7085, -0.3284,  0.2386])tch time: 1.5252\n",
      "Phase: validation   Epoch: 9/30 Loss: 0.3277 Acc: 0.9539        \n",
      "Phase: train Epoch: 10/30 Loss: 0.4273 Acc: 0.8443        \n",
      "tensor([ 0.3371,  0.3006,  0.6053, -0.1429])\n",
      "tensor([ 0.7111,  0.1404, -0.0340,  0.2010])tch time: 1.7198\n",
      "tensor([0.1459, 0.5164, 0.1674, 0.7133])9 Batch time: 1.6007\n",
      "tensor([0.3287, 0.3462, 0.3135, 0.6861])9 Batch time: 1.5313\n",
      "tensor([ 0.1392,  0.1826, -0.3869,  0.2892])tch time: 1.5157\n",
      "tensor([0.7060, 0.3475, 0.0862, 0.7096])9 Batch time: 1.6021\n",
      "tensor([ 0.3102,  0.6174, -0.1583, -0.0565])tch time: 1.5120\n",
      "tensor([0.5853, 0.6712, 0.5816, 0.7137])9 Batch time: 1.5139\n",
      "tensor([ 0.7123,  0.6844,  0.6238, -0.1057])tch time: 1.5669\n",
      "tensor([0.1464, 0.7085, 0.7137, 0.7118])9 Batch time: 1.5222\n",
      "tensor([0.2702, 0.0187, 0.7137, 0.3487])39 Batch time: 1.5099\n",
      "tensor([-0.4317,  0.0969,  0.2557,  0.3300])atch time: 1.5060\n",
      "tensor([0.3472, 0.7072, 0.7129, 0.7128])39 Batch time: 1.5054\n",
      "tensor([-0.0484,  0.2275,  0.7090,  0.2635])atch time: 1.5123\n",
      "tensor([0.7110, 0.6394, 0.1809, 0.7048])39 Batch time: 1.5188\n",
      "tensor([0.6888, 0.3470, 0.3097, 0.7137])39 Batch time: 1.4866\n",
      "tensor([0.2899, 0.2495, 0.6940, 0.2185])39 Batch time: 1.4868\n",
      "tensor([0.7086, 0.7079, 0.6927, 0.3300])39 Batch time: 1.4744\n",
      "tensor([0.6453, 0.2168, 0.3184, 0.2397])39 Batch time: 1.6171\n",
      "tensor([ 0.3149, -0.0254,  0.6988,  0.6873])atch time: 1.5060\n",
      "tensor([0.3449, 0.6936, 0.2672, 0.3306])39 Batch time: 1.5648\n",
      "tensor([0.2157, 0.6348, 0.6971, 0.3444])39 Batch time: 1.5080\n",
      "tensor([ 0.2328,  0.6985,  0.7129, -0.1294])atch time: 1.5252\n",
      "tensor([0.3098, 0.5089, 0.5703, 0.7132])39 Batch time: 1.5061\n",
      "tensor([0.1467, 0.2110, 0.7136, 0.0715])39 Batch time: 1.5030\n",
      "tensor([0.3216, 0.3365, 0.0993, 0.6705])39 Batch time: 1.4809\n",
      "tensor([-0.2814,  0.0095,  0.0748,  0.7054])atch time: 1.5116\n",
      "tensor([-0.2758,  0.7128,  0.5068,  0.3488])atch time: 1.5234\n",
      "tensor([0.6573, 0.3232, 0.3458, 0.1674])39 Batch time: 1.5908\n",
      "tensor([0.5699, 0.3229, 0.7087, 0.7137])39 Batch time: 1.5269\n",
      "tensor([ 0.3192,  0.1932,  0.6850, -0.3907])atch time: 1.4984\n",
      "tensor([0.7044, 0.7108, 0.3269, 0.3439])39 Batch time: 1.5019\n",
      "tensor([0.6799, 0.5326, 0.7055, 0.1897])39 Batch time: 1.5027\n",
      "tensor([0.7138, 0.3364, 0.1708, 0.7004])39 Batch time: 1.5039\n",
      "tensor([0.1683, 0.7018, 0.6772, 0.6995])39 Batch time: 1.5170\n",
      "tensor([ 0.7136, -0.3459,  0.3288, -0.1444])atch time: 1.5093\n",
      "tensor([0.1940, 0.2371, 0.6644, 0.7137])39 Batch time: 1.5268\n",
      "tensor([0.2826, 0.3027, 0.7128, 0.7137])39 Batch time: 1.5081\n",
      "Phase: validation   Epoch: 10/30 Loss: 0.3232 Acc: 0.9408        \n",
      "Phase: train Epoch: 11/30 Loss: 0.3958 Acc: 0.8566        \n",
      "tensor([ 0.6930,  0.5234,  0.6509, -0.2974])\n",
      "tensor([ 0.6607,  0.3491,  0.6982, -0.2548])tch time: 1.5918\n",
      "tensor([0.2039, 0.2014, 0.1605, 0.6266])9 Batch time: 1.5940\n",
      "tensor([0.7183, 0.2687, 0.6623, 0.3411])9 Batch time: 1.5384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1537,  0.2033, -0.0789,  0.3474])tch time: 1.5245\n",
      "tensor([ 0.4247,  0.1025, -0.2019,  0.6436])tch time: 1.5789\n",
      "tensor([ 0.2138,  0.7183, -0.0608,  0.5687])tch time: 1.5731\n",
      "tensor([0.1167, 0.7165, 0.2496, 0.7059])9 Batch time: 1.5718\n",
      "tensor([ 0.7093,  0.2632, -0.1334, -0.4048])tch time: 1.6001\n",
      "tensor([0.7173, 0.3468, 0.3288, 0.3402])9 Batch time: 1.5295\n",
      "tensor([0.6871, 0.6985, 0.7103, 0.3014])39 Batch time: 1.5317\n",
      "tensor([0.1184, 0.3251, 0.0232, 0.2317])39 Batch time: 1.5430\n",
      "tensor([-0.3454,  0.6791,  0.2938,  0.3400])atch time: 1.5728\n",
      "tensor([ 0.7181,  0.3314,  0.7111, -0.0362])atch time: 1.5367\n",
      "tensor([0.2894, 0.6170, 0.2677, 0.3483])39 Batch time: 1.5839\n",
      "tensor([ 0.1542, -0.2320, -0.0893,  0.7159])atch time: 1.5286\n",
      "tensor([0.3121, 0.7155, 0.2701, 0.5670])39 Batch time: 1.6231\n",
      "tensor([0.6048, 0.7123, 0.7160, 0.7155])39 Batch time: 1.7341\n",
      "tensor([0.3254, 0.6495, 0.7180, 0.7042])39 Batch time: 1.7803\n",
      "tensor([0.7182, 0.7182, 0.6162, 0.3261])39 Batch time: 1.5969\n",
      "tensor([-0.4029,  0.2501,  0.1778,  0.3494])atch time: 1.6238\n",
      "tensor([0.2858, 0.1505, 0.6418, 0.3510])39 Batch time: 1.5532\n",
      "tensor([ 0.1245,  0.7178, -0.1699,  0.6713])atch time: 1.5701\n",
      "tensor([0.7180, 0.0201, 0.2928, 0.3516])39 Batch time: 1.5446\n",
      "tensor([0.3440, 0.2141, 0.0581, 0.7180])39 Batch time: 1.5359\n",
      "tensor([0.3252, 0.7170, 0.7029, 0.3510])39 Batch time: 1.5504\n",
      "tensor([0.5425, 0.0289, 0.7166, 0.2363])39 Batch time: 1.5331\n",
      "tensor([0.6982, 0.6761, 0.2246, 0.7143])39 Batch time: 1.5968\n",
      "tensor([0.1040, 0.7148, 0.1793, 0.7181])39 Batch time: 1.6253\n",
      "tensor([-0.2111,  0.3509,  0.2206,  0.2065])atch time: 1.5507\n",
      "tensor([0.7183, 0.5834, 0.6776, 0.5232])39 Batch time: 1.5781\n",
      "tensor([-0.4578,  0.3187,  0.3189,  0.7017])atch time: 1.5650\n",
      "tensor([ 0.3272,  0.7179, -0.0483,  0.0973])atch time: 1.5922\n",
      "tensor([0.3387, 0.3192, 0.7181, 0.5921])39 Batch time: 2.0621\n",
      "tensor([0.5468, 0.3531, 0.6800, 0.6780])39 Batch time: 1.9987\n",
      "tensor([-0.1673,  0.7149, -0.2211,  0.5194])atch time: 1.7679\n",
      "tensor([ 0.2834,  0.3525,  0.6876, -0.3256])atch time: 2.7428\n",
      "tensor([0.3533, 0.6903, 0.7156, 0.7148])39 Batch time: 2.2661\n",
      "Phase: validation   Epoch: 11/30 Loss: 0.3312 Acc: 0.9342        \n",
      "Phase: train Epoch: 12/30 Loss: 0.4016 Acc: 0.8730        \n",
      "tensor([0.7125, 0.6887, 0.2941, 0.7203])\n",
      "tensor([0.0080, 0.3547, 0.7211, 0.7220])9 Batch time: 1.6948\n",
      "tensor([-0.0308, -0.0419,  0.7227,  0.2494])tch time: 1.7493\n",
      "tensor([0.5226, 0.5405, 0.7108, 0.7104])9 Batch time: 1.8189\n",
      "tensor([-0.1615,  0.7142,  0.0821,  0.1094])tch time: 1.8274\n",
      "tensor([-0.1243,  0.2361,  0.7160,  0.2843])tch time: 1.8015\n",
      "tensor([0.6710, 0.2083, 0.3384, 0.3484])9 Batch time: 1.7817\n",
      "tensor([0.1974, 0.7195, 0.1830, 0.7078])9 Batch time: 1.8611\n",
      "tensor([-0.2680,  0.4756,  0.3583,  0.3580])tch time: 1.7210\n",
      "tensor([0.7226, 0.7227, 0.2552, 0.6790])9 Batch time: 1.7931\n",
      "tensor([0.1307, 0.6713, 0.7225, 0.2296])39 Batch time: 1.7295\n",
      "tensor([0.2957, 0.7219, 0.2151, 0.3444])39 Batch time: 1.7651\n",
      "tensor([0.7220, 0.2787, 0.3286, 0.3544])39 Batch time: 1.8448\n",
      "tensor([-0.3844,  0.5653,  0.3461,  0.3152])atch time: 1.7759\n",
      "tensor([0.3085, 0.3540, 0.6757, 0.5910])39 Batch time: 1.8172\n",
      "tensor([0.7212, 0.6912, 0.7231, 0.5863])39 Batch time: 1.8553\n",
      "tensor([0.1646, 0.0184, 0.6926, 0.7166])39 Batch time: 1.7787\n",
      "tensor([0.3246, 0.7053, 0.2360, 0.7019])39 Batch time: 1.7762\n",
      "tensor([0.7226, 0.3435, 0.7125, 0.6589])39 Batch time: 1.8624\n",
      "tensor([ 0.7204,  0.2805,  0.3495, -0.2012])atch time: 1.8694\n",
      "tensor([-0.4391,  0.7206,  0.7205,  0.3299])atch time: 1.7967\n",
      "tensor([ 0.3258, -0.4073,  0.3219,  0.3308])atch time: 1.7449\n",
      "tensor([-0.1421, -0.2464,  0.3572,  0.7147])atch time: 1.7535\n",
      "tensor([0.5353, 0.1323, 0.3349, 0.6949])39 Batch time: 1.8452\n",
      "tensor([ 0.2081, -0.1168,  0.3382,  0.7231])atch time: 1.8777\n",
      "tensor([0.7209, 0.3255, 0.3466, 0.7230])39 Batch time: 1.8518\n",
      "tensor([0.6457, 0.7207, 0.3579, 0.6880])39 Batch time: 1.7571\n",
      "tensor([ 0.3574,  0.3195, -0.0936,  0.6050])atch time: 1.8423\n",
      "tensor([0.2718, 0.3566, 0.2941, 0.0138])39 Batch time: 1.8087\n",
      "tensor([-0.3331,  0.4828,  0.6924,  0.2515])atch time: 1.7789\n",
      "tensor([-0.0753,  0.2231,  0.0438,  0.6246])atch time: 1.8411\n",
      "tensor([0.0944, 0.2675, 0.7232, 0.2250])39 Batch time: 1.8167\n",
      "tensor([0.7229, 0.3120, 0.6967, 0.0743])39 Batch time: 1.8117\n",
      "tensor([0.3525, 0.7232, 0.6918, 0.3311])39 Batch time: 1.8982\n",
      "tensor([ 0.2848,  0.5174, -0.2143,  0.6715])atch time: 1.9002\n",
      "tensor([0.6576, 0.6743, 0.7230, 0.7229])39 Batch time: 1.7686\n",
      "tensor([0.3258, 0.6949, 0.3483, 0.5337])39 Batch time: 1.8079\n",
      "tensor([0.3537, 0.6555, 0.3396, 0.7232])39 Batch time: 1.7684\n",
      "Phase: validation   Epoch: 12/30 Loss: 0.3225 Acc: 0.9474        \n",
      "Phase: train Epoch: 13/30 Loss: 0.3715 Acc: 0.8893        \n",
      "tensor([0.7288, 0.7206, 0.2539, 0.2084])\n",
      "tensor([ 0.3160,  0.3054, -0.2426,  0.6635])tch time: 1.7945\n",
      "tensor([0.5907, 0.2617, 0.2013, 0.3464])9 Batch time: 1.7218\n",
      "tensor([ 0.6982,  0.7144, -0.3282,  0.7241])tch time: 1.7527\n",
      "tensor([ 0.3616, -0.0213,  0.3018,  0.7287])tch time: 1.7510\n",
      "tensor([ 0.7150,  0.3611,  0.7285, -0.1285])tch time: 1.7477\n",
      "tensor([0.6521, 0.4742, 0.3615, 0.7279])9 Batch time: 1.8607\n",
      "tensor([0.7277, 0.7006, 0.6539, 0.7268])9 Batch time: 1.7506\n",
      "tensor([-0.3662,  0.3051,  0.3518,  0.6070])tch time: 1.7281\n",
      "tensor([0.3615, 0.1750, 0.3634, 0.7085])9 Batch time: 1.7227\n",
      "tensor([0.2609, 0.7020, 0.7267, 0.3539])39 Batch time: 1.8022\n",
      "tensor([0.2327, 0.6505, 0.3624, 0.0167])39 Batch time: 1.7594\n",
      "tensor([-0.0942,  0.6718,  0.7287,  0.3497])atch time: 1.7508\n",
      "tensor([0.6831, 0.0643, 0.2436, 0.7248])39 Batch time: 1.7551\n",
      "tensor([-0.2495,  0.7256,  0.3321,  0.3544])atch time: 1.7199\n",
      "tensor([0.1124, 0.0194, 0.6747, 0.1815])39 Batch time: 1.7678\n",
      "tensor([0.2932, 0.3075, 0.7288, 0.3422])39 Batch time: 1.6907\n",
      "tensor([ 0.7285, -0.5040,  0.1654, -0.4376])atch time: 1.7751\n",
      "tensor([0.7272, 0.3601, 0.7200, 0.2964])39 Batch time: 1.7728\n",
      "tensor([0.4210, 0.7283, 0.3496, 0.1633])39 Batch time: 1.7538\n",
      "tensor([0.7157, 0.2722, 0.2216, 0.3031])39 Batch time: 1.7600\n",
      "tensor([0.2996, 0.7282, 0.3213, 0.7121])39 Batch time: 1.7279\n",
      "tensor([0.7265, 0.2112, 0.4196, 0.4887])39 Batch time: 1.7877\n",
      "tensor([ 0.3268,  0.1081,  0.0165, -0.1998])atch time: 1.7117\n",
      "tensor([-0.2327,  0.6751,  0.6663,  0.7281])atch time: 1.8059\n",
      "tensor([0.3336, 0.7205, 0.1188, 0.7221])39 Batch time: 1.8081\n",
      "tensor([0.6462, 0.3628, 0.3342, 0.3849])39 Batch time: 1.8699\n",
      "tensor([ 0.3313,  0.3561,  0.0985, -0.1802])atch time: 1.7356\n",
      "tensor([0.3078, 0.3518, 0.7284, 0.7228])39 Batch time: 1.7664\n",
      "tensor([0.7092, 0.5305, 0.7169, 0.0314])39 Batch time: 1.7761\n",
      "tensor([0.5403, 0.7103, 0.2804, 0.5008])39 Batch time: 1.7728\n",
      "tensor([0.7190, 0.6297, 0.6967, 0.3564])39 Batch time: 1.7573\n",
      "tensor([0.3350, 0.3634, 0.6628, 0.3326])39 Batch time: 1.8370\n",
      "tensor([0.0361, 0.3302, 0.6906, 0.3373])39 Batch time: 1.7748\n",
      "tensor([0.3172, 0.6791, 0.3625, 0.3479])39 Batch time: 1.8093\n",
      "tensor([ 0.2596,  0.6801,  0.3580, -0.4097])atch time: 1.9152\n",
      "tensor([ 0.7238, -0.0736,  0.0226, -0.0024])atch time: 1.7180\n",
      "tensor([0.5715, 0.6660, 0.3476, 0.7231])39 Batch time: 1.7940\n",
      "Phase: validation   Epoch: 13/30 Loss: 0.3243 Acc: 0.9276        \n",
      "Phase: train Epoch: 14/30 Loss: 0.4242 Acc: 0.8156        \n",
      "tensor([0.3268, 0.2725, 0.7237, 0.3411])\n",
      "tensor([0.1017, 0.7290, 0.6960, 0.7175])9 Batch time: 1.7690\n",
      "tensor([0.2503, 0.2423, 0.3439, 0.3637])9 Batch time: 1.6914\n",
      "tensor([0.6847, 0.1098, 0.2275, 0.7292])9 Batch time: 1.7195\n",
      "tensor([0.3436, 0.6613, 0.7290, 0.6474])9 Batch time: 1.7739\n",
      "tensor([0.6007, 0.7285, 0.5039, 0.7276])9 Batch time: 1.8276\n",
      "tensor([0.6997, 0.6930, 0.1428, 0.7288])9 Batch time: 1.7350\n",
      "tensor([0.7237, 0.3297, 0.3627, 0.3638])9 Batch time: 1.7681\n",
      "tensor([0.2739, 0.0375, 0.2167, 0.1507])9 Batch time: 1.7338\n",
      "tensor([0.2243, 0.7145, 0.2765, 0.2552])9 Batch time: 1.8538\n",
      "tensor([ 0.6994,  0.3547, -0.0433, -0.0145])atch time: 1.7445\n",
      "tensor([0.3468, 0.3178, 0.3616, 0.3509])39 Batch time: 1.7598\n",
      "tensor([-0.0120,  0.6787,  0.4881,  0.6897])atch time: 1.6987\n",
      "tensor([0.3448, 0.1347, 0.7069, 0.3627])39 Batch time: 1.7850\n",
      "tensor([0.7213, 0.7187, 0.6784, 0.6730])39 Batch time: 1.8080\n",
      "tensor([ 0.4457, -0.1532,  0.3020,  0.7223])atch time: 1.7409\n",
      "tensor([0.3441, 0.7268, 0.0367, 0.0746])39 Batch time: 1.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3043, 0.3621, 0.3206, 0.7277])39 Batch time: 1.7299\n",
      "tensor([0.7221, 0.5373, 0.5055, 0.0924])39 Batch time: 1.8249\n",
      "tensor([-0.2843,  0.3380,  0.6140,  0.3427])atch time: 1.8274\n",
      "tensor([0.0379, 0.6944, 0.7000, 0.7108])39 Batch time: 1.7499\n",
      "tensor([0.7284, 0.3616, 0.7246, 0.3529])39 Batch time: 1.7939\n",
      "tensor([0.3627, 0.1899, 0.6790, 0.2943])39 Batch time: 1.7852\n",
      "tensor([0.3272, 0.6401, 0.7289, 0.6854])39 Batch time: 1.7719\n",
      "tensor([0.2820, 0.7049, 0.3545, 0.0479])39 Batch time: 1.7851\n",
      "tensor([0.6858, 0.6870, 0.3486, 0.7263])39 Batch time: 1.8260\n",
      "tensor([ 0.7271,  0.3457, -0.3375,  0.7121])atch time: 1.7614\n",
      "tensor([0.1345, 0.7134, 0.7185, 0.7262])39 Batch time: 1.7984\n",
      "tensor([0.3604, 0.1912, 0.3554, 0.7292])39 Batch time: 1.7659\n",
      "tensor([ 0.3226,  0.3544,  0.7262, -0.2711])atch time: 1.7548\n",
      "tensor([0.3041, 0.3318, 0.0483, 0.1795])39 Batch time: 1.8162\n",
      "tensor([-0.1703,  0.5870,  0.5736,  0.5944])atch time: 1.9250\n",
      "tensor([0.3548, 0.6559, 0.7205, 0.3638])39 Batch time: 1.7142\n",
      "tensor([-0.4364, -0.2327,  0.3416,  0.5340])atch time: 1.8209\n",
      "tensor([-0.2472,  0.5232,  0.3238, -0.3895])atch time: 1.7610\n",
      "tensor([0.4888, 0.3158, 0.2528, 0.7284])39 Batch time: 1.7771\n",
      "tensor([0.7285, 0.3472, 0.3295, 0.0363])39 Batch time: 1.7270\n",
      "tensor([-0.4366,  0.2319,  0.7266,  0.7292])atch time: 1.7468\n",
      "Phase: validation   Epoch: 14/30 Loss: 0.3228 Acc: 0.9342        \n",
      "Phase: train Epoch: 15/30 Loss: 0.4486 Acc: 0.7951        \n",
      "tensor([0.7166, 0.0993, 0.2625, 0.0320])\n",
      "tensor([0.3590, 0.0030, 0.6351, 0.3021])9 Batch time: 1.4709\n",
      "tensor([0.3634, 0.3146, 0.6896, 0.2952])9 Batch time: 1.5580\n",
      "tensor([ 0.7269,  0.7018, -0.0450,  0.2978])tch time: 1.5971\n",
      "tensor([ 0.7288,  0.7242, -0.0553, -0.3319])tch time: 1.5543\n",
      "tensor([ 0.2166, -0.1258,  0.6804,  0.6873])tch time: 1.8461\n",
      "tensor([0.4631, 0.4904, 0.6762, 0.7249])9 Batch time: 1.7638\n",
      "tensor([0.3453, 0.7292, 0.3195, 0.7235])9 Batch time: 2.4100\n",
      "tensor([0.7294, 0.2416, 0.3550, 0.7037])9 Batch time: 2.0780\n",
      "tensor([ 0.2135,  0.3515, -0.1119,  0.3587])tch time: 2.6538\n",
      "tensor([0.3559, 0.5875, 0.2411, 0.7217])39 Batch time: 3.9037\n",
      "tensor([-0.3618,  0.3544,  0.0880, -0.2770])atch time: 2.4630\n",
      "tensor([0.3623, 0.3555, 0.2488, 0.3511])39 Batch time: 2.3101\n",
      "tensor([0.6150, 0.3396, 0.6613, 0.3492])39 Batch time: 2.1688\n",
      "tensor([0.3349, 0.7083, 0.7048, 0.7261])39 Batch time: 2.2082\n",
      "tensor([0.4024, 0.2543, 0.5190, 0.7295])39 Batch time: 2.2725\n",
      "tensor([0.0386, 0.2002, 0.6738, 0.6227])39 Batch time: 2.4730\n",
      "tensor([0.7295, 0.7294, 0.6857, 0.2218])39 Batch time: 2.1892\n",
      "tensor([ 0.7289,  0.0112,  0.5171, -0.0455])atch time: 2.4519\n",
      "tensor([0.3311, 0.6592, 0.3635, 0.7247])39 Batch time: 3.0243\n",
      "tensor([0.5593, 0.1969, 0.3139, 0.3642])39 Batch time: 2.5680\n",
      "tensor([ 0.7296, -0.2743,  0.3619,  0.3640])atch time: 2.6223\n",
      "tensor([-0.3712,  0.3450,  0.3240,  0.3617])atch time: 3.0270\n",
      "tensor([0.6804, 0.3619, 0.7279, 0.3449])39 Batch time: 2.4778\n",
      "tensor([0.7135, 0.0194, 0.3107, 0.7291])39 Batch time: 2.6430\n",
      "tensor([ 0.6663,  0.3406, -0.0518,  0.5568])atch time: 2.3754\n",
      "tensor([-0.2931,  0.6494,  0.3397,  0.3551])atch time: 2.4309\n",
      "tensor([0.7292, 0.2708, 0.7237, 0.7266])39 Batch time: 2.6106\n",
      "tensor([0.4555, 0.7291, 0.7062, 0.3231])39 Batch time: 2.6558\n",
      "tensor([0.7273, 0.3471, 0.3386, 0.6511])39 Batch time: 2.5750\n",
      "tensor([-0.0069,  0.3286,  0.2819, -0.0438])atch time: 2.7712\n",
      "tensor([0.7225, 0.3948, 0.3304, 0.2265])39 Batch time: 3.9449\n",
      "tensor([0.7234, 0.2628, 0.3578, 0.3502])39 Batch time: 2.6616\n",
      "tensor([0.1275, 0.6670, 0.6943, 0.7289])39 Batch time: 2.7122\n",
      "tensor([-0.2550, -0.0196,  0.6928, -0.5200])atch time: 2.4425\n",
      "tensor([0.2315, 0.2121, 0.7215, 0.3489])39 Batch time: 2.4618\n",
      "tensor([0.6440, 0.3622, 0.3189, 0.3640])39 Batch time: 2.6123\n",
      "tensor([0.7153, 0.3562, 0.7141, 0.5607])39 Batch time: 2.5580\n",
      "Phase: validation   Epoch: 15/30 Loss: 0.3278 Acc: 0.9145        \n",
      "Phase: train Epoch: 16/30 Loss: 0.3866 Acc: 0.8648        \n",
      "tensor([0.3450, 0.7282, 0.3302, 0.1178])\n",
      "tensor([0.7193, 0.1460, 0.5644, 0.4779])9 Batch time: 0.7595\n",
      "tensor([ 0.7259,  0.2513, -0.0976,  0.3591])tch time: 0.7349\n",
      "tensor([0.6955, 0.0745, 0.7212, 0.3636])9 Batch time: 0.7678\n",
      "tensor([ 0.6733,  0.7058,  0.3644, -0.0482])tch time: 0.7181\n",
      "tensor([ 0.5802,  0.3222, -0.1368,  0.3514])tch time: 0.7156\n",
      "tensor([0.7249, 0.2873, 0.7262, 0.7289])9 Batch time: 0.7332\n",
      "tensor([ 0.3489,  0.1857,  0.7298, -0.0021])tch time: 0.7351\n",
      "tensor([0.0597, 0.2587, 0.6418, 0.6867])9 Batch time: 0.7845\n",
      "tensor([0.3266, 0.3421, 0.7247, 0.7293])9 Batch time: 0.7251\n",
      "tensor([0.7249, 0.3197, 0.3589, 0.3493])39 Batch time: 0.7248\n",
      "tensor([0.6612, 0.1791, 0.7297, 0.6396])39 Batch time: 0.7251\n",
      "tensor([-0.1311,  0.3424,  0.2279,  0.6234])atch time: 0.8692\n",
      "tensor([0.2144, 0.7298, 0.6098, 0.7224])39 Batch time: 0.8334\n",
      "tensor([ 0.6966, -0.0541,  0.0839,  0.7020])atch time: 0.8281\n",
      "tensor([0.1620, 0.3585, 0.3378, 0.6769])39 Batch time: 0.7839\n",
      "tensor([ 0.7300, -0.2291,  0.2267, -0.2082])atch time: 0.7640\n",
      "tensor([0.7295, 0.3641, 0.5795, 0.7220])39 Batch time: 0.7191\n",
      "tensor([0.1685, 0.3390, 0.1220, 0.6433])39 Batch time: 0.7635\n",
      "tensor([-0.0318,  0.5971,  0.2177,  0.3645])atch time: 0.7654\n",
      "tensor([0.2531, 0.5106, 0.7131, 0.2608])39 Batch time: 0.8322\n",
      "tensor([0.3399, 0.3382, 0.7110, 0.3017])39 Batch time: 0.7193\n",
      "tensor([0.2091, 0.7278, 0.3669, 0.7260])39 Batch time: 0.7405\n",
      "tensor([0.7238, 0.2012, 0.7276, 0.3449])39 Batch time: 0.7595\n",
      "tensor([-0.1453,  0.4022,  0.2658,  0.1086])atch time: 0.7374\n",
      "tensor([0.0985, 0.5308, 0.6944, 0.6960])39 Batch time: 0.7845\n",
      "tensor([0.3327, 0.7297, 0.3619, 0.3627])39 Batch time: 0.7863\n",
      "tensor([ 0.3598, -0.3712,  0.2133,  0.6849])atch time: 0.8317\n",
      "tensor([0.5558, 0.6959, 0.7278, 0.6994])39 Batch time: 0.7660\n",
      "tensor([-0.2871,  0.1105,  0.7050,  0.2698])atch time: 0.7904\n",
      "tensor([ 0.0746,  0.1398, -0.3114,  0.7300])atch time: 0.7384\n",
      "tensor([ 0.7227,  0.2859, -0.2850,  0.0661])atch time: 0.7507\n",
      "tensor([ 0.7140,  0.1317, -0.1854,  0.3619])atch time: 0.7949\n",
      "tensor([-0.3465,  0.6621,  0.3067,  0.3496])atch time: 0.7643\n",
      "tensor([-0.0238,  0.7299,  0.2858,  0.5579])atch time: 0.7633\n",
      "tensor([ 0.6945,  0.6504, -0.3073,  0.7065])atch time: 0.7126\n",
      "tensor([0.3408, 0.3178, 0.6796, 0.7297])39 Batch time: 0.7061\n",
      "tensor([ 0.7293, -0.2455,  0.6861,  0.3216])atch time: 0.7235\n",
      "Phase: validation   Epoch: 16/30 Loss: 0.3266 Acc: 0.9276        \n",
      "Phase: train Epoch: 17/30 Loss: 0.3940 Acc: 0.8934        \n",
      "tensor([-0.1849,  0.3545,  0.7002,  0.0649])\n",
      "tensor([ 0.3641,  0.6837,  0.2738, -0.4531])tch time: 0.8459\n",
      "tensor([0.6200, 0.2419, 0.7283, 0.2842])9 Batch time: 0.8031\n",
      "tensor([0.3651, 0.6795, 0.2690, 0.2947])9 Batch time: 0.8186\n",
      "tensor([ 0.4847,  0.2371,  0.1841, -0.2670])tch time: 0.8345\n",
      "tensor([0.6979, 0.2790, 0.7302, 0.2878])9 Batch time: 0.7003\n",
      "tensor([ 0.7032,  0.3463,  0.7296, -0.0766])tch time: 0.7555\n",
      "tensor([ 0.7257, -0.1645,  0.7253,  0.3538])tch time: 0.8373\n",
      "tensor([0.6543, 0.4340, 0.3254, 0.7189])9 Batch time: 0.7615\n",
      "tensor([0.0670, 0.3650, 0.3609, 0.7283])9 Batch time: 0.7201\n",
      "tensor([0.5893, 0.6522, 0.3179, 0.3614])39 Batch time: 0.7482\n",
      "tensor([0.7288, 0.1619, 0.3435, 0.3561])39 Batch time: 0.7640\n",
      "tensor([0.7305, 0.7304, 0.1570, 0.3406])39 Batch time: 0.7713\n",
      "tensor([-0.0179,  0.3582,  0.6349,  0.3646])atch time: 0.7590\n",
      "tensor([0.3609, 0.3623, 0.3545, 0.3614])39 Batch time: 0.7800\n",
      "tensor([0.6328, 0.2791, 0.7145, 0.6509])39 Batch time: 0.7011\n",
      "tensor([0.1922, 0.1646, 0.0113, 0.3610])39 Batch time: 0.7501\n",
      "tensor([0.5631, 0.6295, 0.5029, 0.3646])39 Batch time: 0.7520\n",
      "tensor([0.3242, 0.7303, 0.2306, 0.3964])39 Batch time: 0.7854\n",
      "tensor([ 0.3599,  0.7262, -0.3723,  0.3261])atch time: 0.8039\n",
      "tensor([ 0.2742, -0.2826,  0.3103,  0.3649])atch time: 0.8131\n",
      "tensor([ 0.5359,  0.3390, -0.0544,  0.3219])atch time: 0.8950\n",
      "tensor([0.1172, 0.7070, 0.7259, 0.3586])39 Batch time: 0.8248\n",
      "tensor([0.4946, 0.6131, 0.7295, 0.3485])39 Batch time: 0.7457\n",
      "tensor([-0.3065,  0.6957,  0.3618,  0.3310])atch time: 0.7501\n",
      "tensor([0.3181, 0.6540, 0.3519, 0.2899])39 Batch time: 0.7611\n",
      "tensor([0.2596, 0.2765, 0.7045, 0.6225])39 Batch time: 0.7630\n",
      "tensor([ 0.6992, -0.3227,  0.7199,  0.3634])atch time: 0.7487\n",
      "tensor([0.5947, 0.6950, 0.3492, 0.3180])39 Batch time: 0.7609\n",
      "tensor([0.7288, 0.7304, 0.7141, 0.6964])39 Batch time: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2057,  0.6580,  0.3535,  0.6962])atch time: 0.7696\n",
      "tensor([0.3243, 0.7051, 0.3632, 0.3519])39 Batch time: 0.7721\n",
      "tensor([0.1955, 0.6702, 0.2331, 0.7289])39 Batch time: 0.7701\n",
      "tensor([0.6984, 0.2762, 0.7241, 0.1762])39 Batch time: 0.7670\n",
      "tensor([0.0420, 0.3634, 0.7234, 0.5117])39 Batch time: 0.7783\n",
      "tensor([ 0.3476,  0.5918, -0.0307, -0.3065])atch time: 0.7789\n",
      "tensor([ 0.3443,  0.3467, -0.5212,  0.7302])atch time: 0.8661\n",
      "tensor([ 0.4819,  0.0903, -0.2469,  0.7197])atch time: 0.8921\n",
      "Phase: validation   Epoch: 17/30 Loss: 0.3434 Acc: 0.9013        \n",
      "Phase: train Epoch: 18/30 Loss: 0.3836 Acc: 0.8648        \n",
      "tensor([0.7308, 0.3541, 0.5110, 0.7296])\n",
      "tensor([0.7310, 0.7303, 0.3536, 0.7310])9 Batch time: 0.6966\n",
      "tensor([0.7311, 0.7272, 0.6792, 0.3152])9 Batch time: 0.7675\n",
      "tensor([ 0.3654,  0.3278, -0.0900,  0.6205])tch time: 0.7695\n",
      "tensor([0.2611, 0.3609, 0.7310, 0.0943])9 Batch time: 0.7780\n",
      "tensor([ 0.7076,  0.2697, -0.2912,  0.3311])tch time: 0.8175\n",
      "tensor([0.3592, 0.6409, 0.2664, 0.3686])9 Batch time: 0.7206\n",
      "tensor([ 0.3628,  0.0464, -0.1067,  0.6523])tch time: 0.7249\n",
      "tensor([0.3223, 0.3638, 0.7296, 0.0629])9 Batch time: 0.7518\n",
      "tensor([0.7048, 0.2642, 0.7138, 0.7310])9 Batch time: 0.8169\n",
      "tensor([0.6446, 0.2952, 0.6687, 0.1540])39 Batch time: 0.7528\n",
      "tensor([0.3282, 0.3045, 0.7310, 0.3409])39 Batch time: 0.8074\n",
      "tensor([0.7310, 0.2283, 0.3376, 0.6470])39 Batch time: 0.7196\n",
      "tensor([0.1711, 0.3292, 0.7209, 0.5925])39 Batch time: 0.7807\n",
      "tensor([-0.0979,  0.3439,  0.2004,  0.2276])atch time: 0.6952\n",
      "tensor([0.7252, 0.2677, 0.7185, 0.7165])39 Batch time: 0.9376\n",
      "tensor([0.2986, 0.7293, 0.7300, 0.7137])39 Batch time: 0.8781\n",
      "tensor([0.7024, 0.6895, 0.7305, 0.3495])39 Batch time: 0.8135\n",
      "tensor([0.7024, 0.6889, 0.7311, 0.7268])39 Batch time: 0.8308\n",
      "tensor([0.7223, 0.4729, 0.3614, 0.7299])39 Batch time: 0.7411\n",
      "tensor([-0.2779, -0.0450,  0.7307,  0.7034])atch time: 0.8901\n",
      "tensor([0.2706, 0.7309, 0.3221, 0.7008])39 Batch time: 0.8647\n",
      "tensor([ 0.7236,  0.2082,  0.3146, -0.2646])atch time: 0.8623\n",
      "tensor([ 0.2051,  0.0329, -0.2378,  0.4418])atch time: 0.8971\n",
      "tensor([0.6360, 0.3381, 0.3484, 0.3117])39 Batch time: 0.7403\n",
      "tensor([ 0.1655,  0.2000, -0.4027,  0.1539])atch time: 0.8074\n",
      "tensor([-0.4564,  0.3645,  0.3312,  0.2211])atch time: 0.8029\n",
      "tensor([0.1148, 0.6874, 0.2465, 0.5903])39 Batch time: 0.7879\n",
      "tensor([-0.0470,  0.7272,  0.7308,  0.7285])atch time: 0.7559\n",
      "tensor([0.0948, 0.7222, 0.7309, 0.6102])39 Batch time: 0.8293\n",
      "tensor([ 0.6962,  0.1672, -0.2525,  0.1863])atch time: 0.7840\n",
      "tensor([0.3651, 0.7247, 0.3147, 0.3360])39 Batch time: 0.8072\n",
      "tensor([ 0.5436, -0.0245, -0.3362, -0.0534])atch time: 0.9639\n",
      "tensor([ 0.7155,  0.7300,  0.3522, -0.2900])atch time: 0.8958\n",
      "tensor([0.7282, 0.0475, 0.0854, 0.0504])39 Batch time: 0.8630\n",
      "tensor([0.3648, 0.7257, 0.2609, 0.6946])39 Batch time: 0.8567\n",
      "tensor([-0.4581,  0.1661,  0.7292,  0.7091])atch time: 0.8588\n",
      "tensor([ 0.2017,  0.7131, -0.0062,  0.7239])atch time: 0.8081\n",
      "Phase: validation   Epoch: 18/30 Loss: 0.3163 Acc: 0.9474        \n",
      "Phase: train Epoch: 19/30 Loss: 0.4222 Acc: 0.8197        \n",
      "tensor([ 0.6383,  0.2473,  0.6484, -0.1497])\n",
      "tensor([0.3279, 0.4604, 0.7314, 0.2466])9 Batch time: 1.2004\n",
      "tensor([-0.3778, -0.0826,  0.7125,  0.6752])tch time: 1.2184\n",
      "tensor([0.2496, 0.3819, 0.7228, 0.3560])9 Batch time: 1.1781\n",
      "tensor([-0.0217,  0.3030, -0.1964,  0.3363])tch time: 1.2397\n",
      "tensor([0.7126, 0.7313, 0.3658, 0.3099])9 Batch time: 1.3188\n",
      "tensor([ 0.6761, -0.3875,  0.7240,  0.7309])tch time: 1.0693\n",
      "tensor([0.7180, 0.6443, 0.2200, 0.7261])9 Batch time: 1.1826\n",
      "tensor([0.3363, 0.3650, 0.2387, 0.0467])9 Batch time: 1.1013\n",
      "tensor([0.1739, 0.3567, 0.0548, 0.7140])9 Batch time: 1.1954\n",
      "tensor([-0.0661,  0.3302,  0.7310,  0.7080])atch time: 1.2316\n",
      "tensor([0.2740, 0.7278, 0.2308, 0.3558])39 Batch time: 1.4165\n",
      "tensor([0.3593, 0.5380, 0.3436, 0.6670])39 Batch time: 1.2071\n",
      "tensor([0.7315, 0.5640, 0.7217, 0.6837])39 Batch time: 1.1769\n",
      "tensor([ 0.6658, -0.1475,  0.3408,  0.7298])atch time: 1.1810\n",
      "tensor([ 0.3651, -0.0112,  0.3390,  0.0803])atch time: 1.1678\n",
      "tensor([0.1470, 0.3648, 0.6159, 0.3388])39 Batch time: 1.2011\n",
      "tensor([0.7036, 0.2478, 0.3660, 0.3642])39 Batch time: 1.1592\n",
      "tensor([-0.2483,  0.7280,  0.6574,  0.3628])atch time: 1.1814\n",
      "tensor([ 0.3028,  0.1530,  0.7313, -0.0057])atch time: 1.1431\n",
      "tensor([0.3052, 0.7028, 0.7310, 0.1020])39 Batch time: 1.0777\n",
      "tensor([0.6893, 0.3527, 0.3014, 0.2138])39 Batch time: 1.0767\n",
      "tensor([-0.3298, -0.3287, -0.3485,  0.7086])atch time: 1.1103\n",
      "tensor([0.6960, 0.6536, 0.3550, 0.7063])39 Batch time: 1.1638\n",
      "tensor([0.6777, 0.7314, 0.3639, 0.3394])39 Batch time: 1.1546\n",
      "tensor([0.3342, 0.3464, 0.2936, 0.3591])39 Batch time: 1.5122\n",
      "tensor([ 0.1918, -0.4754,  0.6024,  0.3568])atch time: 2.0733\n",
      "tensor([0.6751, 0.2767, 0.3556, 0.7311])39 Batch time: 1.5600\n",
      "tensor([0.6945, 0.3188, 0.5314, 0.7267])39 Batch time: 1.2325\n",
      "tensor([0.6988, 0.5090, 0.6279, 0.3579])39 Batch time: 1.1210\n",
      "tensor([0.3492, 0.3647, 0.7138, 0.7311])39 Batch time: 1.1565\n",
      "tensor([0.0038, 0.2382, 0.7266, 0.7264])39 Batch time: 1.0497\n",
      "tensor([ 0.2751,  0.7282,  0.3548, -0.0723])atch time: 1.1188\n",
      "tensor([0.7303, 0.7293, 0.3610, 0.4445])39 Batch time: 1.2127\n",
      "tensor([-0.2351,  0.5654,  0.3637,  0.3587])atch time: 1.3455\n",
      "tensor([ 0.2896,  0.3658, -0.0089,  0.5507])atch time: 1.3564\n",
      "tensor([0.3116, 0.7311, 0.1963, 0.7287])39 Batch time: 1.0211\n",
      "tensor([-0.2327,  0.7291,  0.7250,  0.3047])atch time: 0.9615\n",
      "Phase: validation   Epoch: 19/30 Loss: 0.3262 Acc: 0.9211        \n",
      "Phase: train Epoch: 20/30 Loss: 0.3757 Acc: 0.8770        \n",
      "tensor([-0.0384,  0.7308,  0.7318,  0.3594])\n",
      "tensor([-0.0739, -0.5273,  0.7110,  0.2963])tch time: 0.8924\n",
      "tensor([ 0.4052, -0.1453,  0.7267,  0.3382])tch time: 1.0892\n",
      "tensor([0.5627, 0.6201, 0.7021, 0.5020])9 Batch time: 1.1732\n",
      "tensor([0.6613, 0.3446, 0.7236, 0.3530])9 Batch time: 1.2039\n",
      "tensor([ 0.6274, -0.2925,  0.7192, -0.3287])tch time: 1.2037\n",
      "tensor([0.1170, 0.3224, 0.7290, 0.6963])9 Batch time: 1.2998\n",
      "tensor([0.7318, 0.2677, 0.6756, 0.3143])9 Batch time: 1.1831\n",
      "tensor([0.1042, 0.7243, 0.3639, 0.7280])9 Batch time: 1.2560\n",
      "tensor([0.0359, 0.7316, 0.3080, 0.0080])9 Batch time: 1.2403\n",
      "tensor([0.7310, 0.2966, 0.1342, 0.7301])39 Batch time: 1.2408\n",
      "tensor([-0.4231,  0.3647,  0.6804, -0.2727])atch time: 1.3106\n",
      "tensor([0.7253, 0.3556, 0.7223, 0.7319])39 Batch time: 1.1436\n",
      "tensor([0.1994, 0.7320, 0.4593, 0.6202])39 Batch time: 1.1370\n",
      "tensor([0.4184, 0.6101, 0.6789, 0.3506])39 Batch time: 1.1891\n",
      "tensor([-0.1236, -0.3766, -0.0958,  0.7109])atch time: 1.0872\n",
      "tensor([-0.2585,  0.3577,  0.2104,  0.2798])atch time: 1.0296\n",
      "tensor([-0.0754,  0.3423,  0.7226,  0.5150])atch time: 1.3530\n",
      "tensor([ 0.0659, -0.0907,  0.3662,  0.3214])atch time: 1.7099\n",
      "tensor([0.5295, 0.3664, 0.3333, 0.4934])39 Batch time: 1.1163\n",
      "tensor([0.2671, 0.7070, 0.2076, 0.2556])39 Batch time: 1.0262\n",
      "tensor([0.7319, 0.2381, 0.3322, 0.7032])39 Batch time: 1.1191\n",
      "tensor([0.3616, 0.3656, 0.3565, 0.2167])39 Batch time: 1.1570\n",
      "tensor([0.7308, 0.2925, 0.2116, 0.7274])39 Batch time: 1.1493\n",
      "tensor([0.3544, 0.3660, 0.7318, 0.5683])39 Batch time: 1.1239\n",
      "tensor([0.7319, 0.3651, 0.3566, 0.6954])39 Batch time: 1.0462\n",
      "tensor([ 0.3374,  0.3322, -0.0126,  0.7318])atch time: 1.0231\n",
      "tensor([0.3485, 0.3448, 0.7313, 0.7169])39 Batch time: 1.0705\n",
      "tensor([0.7244, 0.3496, 0.7028, 0.6938])39 Batch time: 1.0103\n",
      "tensor([0.3435, 0.3641, 0.7320, 0.3700])39 Batch time: 1.0426\n",
      "tensor([0.0260, 0.2585, 0.6953, 0.3620])39 Batch time: 1.1856\n",
      "tensor([0.6940, 0.6874, 0.1714, 0.2201])39 Batch time: 1.2361\n",
      "tensor([ 0.6665,  0.3206,  0.2165, -0.3171])atch time: 1.1840\n",
      "tensor([ 0.7130,  0.2483, -0.1297,  0.6601])atch time: 1.1327\n",
      "tensor([0.3643, 0.7302, 0.2994, 0.6574])39 Batch time: 1.0980\n",
      "tensor([0.7289, 0.2016, 0.3458, 0.7105])39 Batch time: 1.1394\n",
      "tensor([0.0333, 0.3536, 0.6988, 0.7233])39 Batch time: 1.1300\n",
      "tensor([ 0.5386, -0.0130,  0.2969,  0.3351])atch time: 1.2309\n",
      "Phase: validation   Epoch: 20/30 Loss: 0.3225 Acc: 0.9276        \n",
      "Phase: train Epoch: 21/30 Loss: 0.4106 Acc: 0.8402        \n",
      "tensor([0.7321, 0.3358, 0.7179, 0.3324])\n",
      "tensor([0.7319, 0.3658, 0.7301, 0.2869])9 Batch time: 0.8756\n",
      "tensor([0.3399, 0.1999, 0.3221, 0.3580])9 Batch time: 1.0423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0678, -0.4121,  0.2895, -0.2216])tch time: 1.1527\n",
      "tensor([0.7321, 0.3317, 0.7076, 0.7321])9 Batch time: 1.3927\n",
      "tensor([ 0.2993,  0.7002, -0.3002,  0.7211])tch time: 1.0706\n",
      "tensor([0.3665, 0.2386, 0.3627, 0.3646])9 Batch time: 1.2775\n",
      "tensor([0.2794, 0.1289, 0.7313, 0.3491])9 Batch time: 1.3835\n",
      "tensor([ 0.7305,  0.7227,  0.5898, -0.4890])tch time: 1.4120\n",
      "tensor([-0.0459,  0.3079, -0.3584,  0.6669])tch time: 1.1759\n",
      "tensor([0.7300, 0.7275, 0.3305, 0.6258])39 Batch time: 1.0890\n",
      "tensor([0.7317, 0.3585, 0.7052, 0.3465])39 Batch time: 1.0980\n",
      "tensor([ 0.7320,  0.3423,  0.5565, -0.0072])atch time: 1.1071\n",
      "tensor([0.2187, 0.3574, 0.7292, 0.2915])39 Batch time: 1.2086\n",
      "tensor([0.3347, 0.7245, 0.7087, 0.3487])39 Batch time: 1.1623\n",
      "tensor([0.6166, 0.7289, 0.1657, 0.3773])39 Batch time: 1.2492\n",
      "tensor([ 0.5195, -0.2771,  0.7299,  0.7302])atch time: 1.1969\n",
      "tensor([0.1452, 0.6441, 0.6946, 0.6862])39 Batch time: 1.2074\n",
      "tensor([0.7320, 0.1309, 0.7250, 0.0084])39 Batch time: 1.1765\n",
      "tensor([0.6594, 0.6602, 0.3540, 0.6676])39 Batch time: 1.1799\n",
      "tensor([0.3604, 0.7319, 0.0153, 0.7319])39 Batch time: 1.1169\n",
      "tensor([0.2589, 0.7311, 0.7283, 0.7307])39 Batch time: 1.1230\n",
      "tensor([0.0706, 0.2941, 0.7050, 0.2315])39 Batch time: 1.0890\n",
      "tensor([0.7320, 0.7209, 0.6983, 0.1385])39 Batch time: 1.0930\n",
      "tensor([ 0.6373, -0.0952,  0.5702,  0.6871])atch time: 1.2710\n",
      "tensor([-0.4241,  0.7119,  0.7048,  0.1862])atch time: 1.0472\n",
      "tensor([0.2647, 0.7317, 0.4936, 0.2610])39 Batch time: 1.1310\n",
      "tensor([ 0.0323,  0.2248, -0.0121,  0.6865])atch time: 1.1122\n",
      "tensor([ 0.3524, -0.0497,  0.7319,  0.7225])atch time: 1.1730\n",
      "tensor([ 0.7319,  0.7183, -0.1665,  0.3274])atch time: 1.1484\n",
      "tensor([0.3431, 0.7316, 0.7271, 0.1337])39 Batch time: 1.1860\n",
      "tensor([ 0.1764, -0.3424, -0.1184,  0.7156])atch time: 1.2393\n",
      "tensor([ 0.3470,  0.1407, -0.0667,  0.2544])atch time: 1.2031\n",
      "tensor([0.7000, 0.7103, 0.3612, 0.7171])39 Batch time: 1.2068\n",
      "tensor([-0.2238,  0.3289,  0.2132,  0.5135])atch time: 1.1661\n",
      "tensor([ 0.6626,  0.0442,  0.3660, -0.1739])atch time: 1.1460\n",
      "tensor([0.2652, 0.3310, 0.5558, 0.3626])39 Batch time: 1.0485\n",
      "tensor([0.7264, 0.2257, 0.3458, 0.2306])39 Batch time: 1.0570\n",
      "Phase: validation   Epoch: 21/30 Loss: 0.3182 Acc: 0.9408        \n",
      "Phase: train Epoch: 22/30 Loss: 0.4132 Acc: 0.8607        \n",
      "tensor([-0.1146,  0.3407,  0.2377,  0.6918])\n",
      "tensor([0.2596, 0.7257, 0.2316, 0.6201])9 Batch time: 1.0395\n",
      "tensor([ 0.7159,  0.0830, -0.1373,  0.3605])tch time: 1.1226\n",
      "tensor([0.7286, 0.7320, 0.3480, 0.2895])9 Batch time: 1.1600\n",
      "tensor([0.1384, 0.6269, 0.2526, 0.6768])9 Batch time: 1.4001\n",
      "tensor([ 0.7301, -0.3169,  0.3569,  0.6379])tch time: 1.1249\n",
      "tensor([-0.3823,  0.4833,  0.6333,  0.6261])tch time: 1.0853\n",
      "tensor([0.3393, 0.7090, 0.7315, 0.3491])9 Batch time: 1.0477\n",
      "tensor([0.2360, 0.7312, 0.3427, 0.3446])9 Batch time: 1.1786\n",
      "tensor([0.7221, 0.3269, 0.3372, 0.3364])9 Batch time: 1.1660\n",
      "tensor([0.3531, 0.7318, 0.7253, 0.3425])39 Batch time: 1.0351\n",
      "tensor([ 0.6846,  0.7239, -0.2697,  0.2851])atch time: 0.9607\n",
      "tensor([ 0.1054, -0.2606,  0.7315,  0.3505])atch time: 0.9377\n",
      "tensor([ 0.2052,  0.3445,  0.3170, -0.0224])atch time: 0.9651\n",
      "tensor([0.3313, 0.7321, 0.3655, 0.6906])39 Batch time: 1.0183\n",
      "tensor([0.6536, 0.7320, 0.7066, 0.7041])39 Batch time: 1.0731\n",
      "tensor([0.3111, 0.7322, 0.3652, 0.2922])39 Batch time: 1.0559\n",
      "tensor([0.3577, 0.2040, 0.3627, 0.7052])39 Batch time: 1.0687\n",
      "tensor([ 0.4992,  0.0392, -0.2650,  0.7281])atch time: 0.8323\n",
      "tensor([0.7053, 0.3630, 0.7312, 0.2813])39 Batch time: 0.7991\n",
      "tensor([ 0.7010,  0.3665,  0.3619, -0.1386])atch time: 0.7810\n",
      "tensor([ 0.7279, -0.1433,  0.1297,  0.2748])atch time: 0.7655\n",
      "tensor([0.5187, 0.1104, 0.3568, 0.7294])39 Batch time: 0.8031\n",
      "tensor([ 0.3657,  0.7132, -0.0767,  0.7100])atch time: 0.8397\n",
      "tensor([0.7297, 0.7321, 0.5624, 0.7275])39 Batch time: 0.8301\n",
      "tensor([0.5475, 0.3400, 0.7159, 0.7321])39 Batch time: 0.8499\n",
      "tensor([0.7274, 0.3645, 0.6949, 0.3648])39 Batch time: 0.9965\n",
      "tensor([0.7175, 0.3448, 0.0042, 0.1900])39 Batch time: 0.9066\n",
      "tensor([-0.0462,  0.7208,  0.3112,  0.2028])atch time: 0.9918\n",
      "tensor([ 0.7243,  0.3499, -0.3455,  0.6191])atch time: 0.9322\n",
      "tensor([-0.0087,  0.4431,  0.3525,  0.0140])atch time: 1.2331\n",
      "tensor([ 0.5575,  0.7314, -0.2370,  0.2692])atch time: 1.1279\n",
      "tensor([0.2994, 0.2874, 0.2589, 0.1620])39 Batch time: 1.1050\n",
      "tensor([0.6796, 0.2589, 0.7150, 0.7052])39 Batch time: 1.1652\n",
      "tensor([-0.3313,  0.7319,  0.7056,  0.7320])atch time: 1.1025\n",
      "tensor([0.5213, 0.6738, 0.7321, 0.0523])39 Batch time: 1.0796\n",
      "tensor([0.7276, 0.7294, 0.7307, 0.2056])39 Batch time: 1.1087\n",
      "tensor([ 0.3544,  0.0729,  0.3664, -0.0956])atch time: 1.1903\n",
      "Phase: validation   Epoch: 22/30 Loss: 0.3164 Acc: 0.9342        \n",
      "Phase: train Epoch: 23/30 Loss: 0.3876 Acc: 0.8730        \n",
      "tensor([0.3645, 0.7079, 0.2548, 0.3614])\n",
      "tensor([ 0.7069,  0.7063, -0.2588,  0.2997])tch time: 0.9848\n",
      "tensor([ 0.3431, -0.1699,  0.3511,  0.7077])tch time: 1.0592\n",
      "tensor([0.7151, 0.6695, 0.3605, 0.3453])9 Batch time: 1.0484\n",
      "tensor([0.7257, 0.0855, 0.7183, 0.6443])9 Batch time: 0.9921\n",
      "tensor([0.0364, 0.3462, 0.2561, 0.3657])9 Batch time: 1.0361\n",
      "tensor([0.5645, 0.0662, 0.1248, 0.7311])9 Batch time: 1.3469\n",
      "tensor([0.7319, 0.3492, 0.7153, 0.6735])9 Batch time: 0.9734\n",
      "tensor([0.7243, 0.3404, 0.6435, 0.7309])9 Batch time: 0.8404\n",
      "tensor([ 0.2334,  0.3662,  0.7243, -0.0250])tch time: 1.0716\n",
      "tensor([ 0.3502, -0.4929,  0.3567,  0.3618])atch time: 1.0214\n",
      "tensor([0.3652, 0.3510, 0.6844, 0.3422])39 Batch time: 1.0436\n",
      "tensor([0.3040, 0.6844, 0.7271, 0.3604])39 Batch time: 1.0973\n",
      "tensor([0.3666, 0.7318, 0.2633, 0.3785])39 Batch time: 1.1364\n",
      "tensor([0.1781, 0.0333, 0.3264, 0.7218])39 Batch time: 0.9844\n",
      "tensor([0.2619, 0.3621, 0.3661, 0.0615])39 Batch time: 0.8194\n",
      "tensor([0.6503, 0.3443, 0.7307, 0.5992])39 Batch time: 0.8363\n",
      "tensor([0.0350, 0.5930, 0.3594, 0.3664])39 Batch time: 0.8248\n",
      "tensor([ 0.3666, -0.3434,  0.7320,  0.6708])atch time: 0.9470\n",
      "tensor([ 0.6882,  0.3550, -0.2070,  0.3024])atch time: 1.0084\n",
      "tensor([0.4736, 0.6398, 0.7315, 0.2883])39 Batch time: 1.0765\n",
      "tensor([0.5758, 0.7043, 0.6259, 0.1596])39 Batch time: 1.1576\n",
      "tensor([0.6008, 0.3591, 0.7259, 0.6887])39 Batch time: 1.0873\n",
      "tensor([0.7315, 0.4510, 0.1013, 0.7196])39 Batch time: 1.4423\n",
      "tensor([0.7027, 0.3375, 0.6647, 0.2747])39 Batch time: 1.3543\n",
      "tensor([-0.2236, -0.0992,  0.3577, -0.2160])atch time: 1.2374\n",
      "tensor([ 0.2737,  0.2132,  0.7012, -0.1111])atch time: 1.1067\n",
      "tensor([0.7296, 0.7270, 0.7112, 0.3186])39 Batch time: 1.1948\n",
      "tensor([0.4552, 0.3359, 0.3332, 0.1043])39 Batch time: 1.1263\n",
      "tensor([0.3516, 0.7159, 0.2245, 0.7001])39 Batch time: 1.0073\n",
      "tensor([0.7320, 0.3583, 0.7315, 0.3658])39 Batch time: 0.9571\n",
      "tensor([ 0.3316,  0.6671,  0.3423, -0.2931])atch time: 0.9134\n",
      "tensor([0.5827, 0.7287, 0.2776, 0.4796])39 Batch time: 0.9371\n",
      "tensor([ 0.3619,  0.3200,  0.7229, -0.2391])atch time: 0.9192\n",
      "tensor([ 0.7271, -0.4626,  0.3245,  0.3591])atch time: 0.8822\n",
      "tensor([0.3658, 0.0720, 0.0643, 0.6765])39 Batch time: 0.8877\n",
      "tensor([0.2928, 0.3567, 0.2568, 0.2039])39 Batch time: 0.8937\n",
      "tensor([0.3104, 0.7322, 0.2849, 0.5670])39 Batch time: 0.9183\n",
      "Phase: validation   Epoch: 23/30 Loss: 0.3343 Acc: 0.9145        \n",
      "Phase: train Epoch: 24/30 Loss: 0.3693 Acc: 0.8852        \n",
      "tensor([0.3437, 0.7291, 0.7304, 0.7289])\n",
      "tensor([ 7.3018e-01,  2.9243e-01,  6.1093e-01, -2.7343e-04])\n",
      "tensor([0.2784, 0.2788, 0.5538, 0.1737])9 Batch time: 0.8947\n",
      "tensor([0.6126, 0.7300, 0.3635, 0.3549])9 Batch time: 0.9233\n",
      "tensor([0.6921, 0.7275, 0.7104, 0.6146])9 Batch time: 0.8985\n",
      "tensor([ 0.3122,  0.1970, -0.1766,  0.2088])tch time: 0.8932\n",
      "tensor([ 0.7071, -0.0365,  0.6425,  0.7308])tch time: 0.8798\n",
      "tensor([0.7319, 0.3661, 0.2210, 0.3649])9 Batch time: 0.8402\n",
      "tensor([0.7311, 0.1365, 0.1794, 0.3249])9 Batch time: 0.8928\n",
      "tensor([0.7262, 0.7137, 0.7053, 0.2805])9 Batch time: 0.8852\n",
      "tensor([0.7312, 0.6734, 0.0076, 0.7256])39 Batch time: 0.8488\n",
      "tensor([0.7182, 0.3641, 0.3384, 0.6747])39 Batch time: 0.8533\n",
      "tensor([ 0.3531, -0.3580,  0.3198,  0.0157])atch time: 0.8797\n",
      "tensor([0.1874, 0.7322, 0.7322, 0.7230])39 Batch time: 0.8699\n",
      "tensor([ 0.3403,  0.2852, -0.3959, -0.3814])atch time: 0.8354\n",
      "tensor([ 0.7251,  0.4227,  0.7323, -0.4068])atch time: 0.8242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2776, 0.3503, 0.2270, 0.3147])39 Batch time: 0.8483\n",
      "tensor([ 0.6502,  0.7322, -0.1401,  0.3620])atch time: 0.8921\n",
      "tensor([0.3354, 0.5942, 0.7322, 0.1931])39 Batch time: 0.8999\n",
      "tensor([-0.0458,  0.5419, -0.3221,  0.6043])atch time: 0.8924\n",
      "tensor([ 0.3536, -0.3119,  0.7214,  0.7322])atch time: 1.0969\n",
      "tensor([0.7271, 0.3591, 0.6115, 0.2468])39 Batch time: 0.8958\n",
      "tensor([-0.1317,  0.7317,  0.3492,  0.3013])atch time: 0.8877\n",
      "tensor([0.3144, 0.7322, 0.3316, 0.7146])39 Batch time: 0.8616\n",
      "tensor([0.3366, 0.7317, 0.0751, 0.5817])39 Batch time: 0.9298\n",
      "tensor([-0.1351,  0.7321,  0.1689,  0.7097])atch time: 0.8528\n",
      "tensor([0.0950, 0.3242, 0.3290, 0.3629])39 Batch time: 0.8488\n",
      "tensor([0.6849, 0.1434, 0.6139, 0.0972])39 Batch time: 0.8785\n",
      "tensor([ 0.7261,  0.6674, -0.0045,  0.0534])atch time: 0.8757\n",
      "tensor([0.3595, 0.6699, 0.1420, 0.7322])39 Batch time: 0.8625\n",
      "tensor([0.7319, 0.1985, 0.6988, 0.3666])39 Batch time: 0.8695\n",
      "tensor([ 0.0620,  0.3660, -0.2247,  0.7307])atch time: 0.9155\n",
      "tensor([-0.2641,  0.0760,  0.1227,  0.3385])atch time: 0.9131\n",
      "tensor([ 0.3535,  0.1561,  0.3413, -0.0046])atch time: 0.9084\n",
      "tensor([-0.2512,  0.6854,  0.7191,  0.7242])atch time: 0.9480\n",
      "tensor([0.3283, 0.7071, 0.1787, 0.7313])39 Batch time: 0.9491\n",
      "tensor([0.7011, 0.7320, 0.6960, 0.7208])39 Batch time: 1.0538\n",
      "tensor([-0.3464,  0.1371,  0.7079,  0.6818])atch time: 0.9254\n",
      "Phase: validation   Epoch: 24/30 Loss: 0.3218 Acc: 0.9474        \n",
      "Phase: train Epoch: 25/30 Loss: 0.3769 Acc: 0.8730        \n",
      "tensor([0.7136, 0.3588, 0.7290, 0.7313])\n",
      "tensor([0.3418, 0.7309, 0.5530, 0.3506])9 Batch time: 1.1050\n",
      "tensor([0.2718, 0.2162, 0.3534, 0.1743])9 Batch time: 1.1270\n",
      "tensor([ 0.2735,  0.3661, -0.2011,  0.6270])tch time: 1.1231\n",
      "tensor([0.6752, 0.1912, 0.6708, 0.3641])9 Batch time: 1.1474\n",
      "tensor([0.3372, 0.7320, 0.7214, 0.1581])9 Batch time: 1.1791\n",
      "tensor([0.3319, 0.6424, 0.2444, 0.3615])9 Batch time: 1.1884\n",
      "tensor([ 0.7208,  0.1823,  0.3533, -0.1634])tch time: 1.2000\n",
      "tensor([ 0.2697,  0.7299, -0.0680,  0.7088])tch time: 1.1504\n",
      "tensor([0.3660, 0.7269, 0.3574, 0.5266])9 Batch time: 1.2368\n",
      "tensor([ 0.3665,  0.3624,  0.7158, -0.2812])atch time: 1.4315\n",
      "tensor([0.2428, 0.6171, 0.6604, 0.3661])39 Batch time: 1.3007\n",
      "tensor([-0.2773,  0.3448,  0.2547,  0.3561])atch time: 1.0832\n",
      "tensor([ 0.6963, -0.2628,  0.3311,  0.3028])atch time: 1.3968\n",
      "tensor([0.2139, 0.2283, 0.4621, 0.6837])39 Batch time: 1.5377\n",
      "tensor([0.3164, 0.1642, 0.3486, 0.3373])39 Batch time: 1.6219\n",
      "tensor([ 0.1195, -0.3079,  0.3155,  0.6869])atch time: 1.7420\n",
      "tensor([ 0.3526,  0.3030, -0.3911, -0.1258])atch time: 1.6332\n",
      "tensor([0.7320, 0.7316, 0.6532, 0.1500])39 Batch time: 1.5414\n",
      "tensor([0.3667, 0.4441, 0.7321, 0.3622])39 Batch time: 1.6778\n",
      "tensor([-0.1264,  0.6957,  0.6750,  0.7182])atch time: 1.5956\n",
      "tensor([-0.5123,  0.7301,  0.7313,  0.0009])atch time: 1.3954\n",
      "tensor([ 0.3591, -0.0415,  0.6047,  0.7266])atch time: 1.4051\n",
      "tensor([ 0.6018,  0.2517, -0.2532,  0.2633])atch time: 1.7059\n",
      "tensor([-0.0824,  0.5397,  0.3452,  0.7147])atch time: 1.2801\n",
      "tensor([0.3787, 0.7236, 0.3034, 0.7238])39 Batch time: 1.3390\n",
      "tensor([0.2703, 0.3117, 0.5363, 0.2743])39 Batch time: 1.1838\n",
      "tensor([ 0.2033,  0.3071,  0.5209, -0.0339])atch time: 1.4332\n",
      "tensor([0.3127, 0.3569, 0.5938, 0.7286])39 Batch time: 1.2577\n",
      "tensor([ 0.7323,  0.7313, -0.1112,  0.1675])atch time: 1.2160\n",
      "tensor([0.7271, 0.6916, 0.5352, 0.6161])39 Batch time: 0.9985\n",
      "tensor([0.3636, 0.1280, 0.3486, 0.6860])39 Batch time: 0.9694\n",
      "tensor([0.3413, 0.7049, 0.0409, 0.3200])39 Batch time: 0.9432\n",
      "tensor([0.5974, 0.3655, 0.7315, 0.6726])39 Batch time: 0.9158\n",
      "tensor([-0.2795,  0.6442,  0.3632,  0.6107])atch time: 0.9216\n",
      "tensor([0.6980, 0.3440, 0.3030, 0.7221])39 Batch time: 0.9386\n",
      "tensor([-0.3571,  0.7145,  0.3651, -0.1024])atch time: 0.9209\n",
      "tensor([ 0.3539,  0.1836, -0.5378,  0.3646])atch time: 1.0034\n",
      "Phase: validation   Epoch: 25/30 Loss: 0.3451 Acc: 0.8947        \n",
      "Phase: train Epoch: 26/30 Loss: 0.4207 Acc: 0.8402        \n",
      "tensor([0.3562, 0.2583, 0.1200, 0.7062])\n",
      "tensor([0.3359, 0.1096, 0.5707, 0.1983])9 Batch time: 0.9087\n",
      "tensor([0.2241, 0.3536, 0.7304, 0.7295])9 Batch time: 1.2000\n",
      "tensor([0.7220, 0.5307, 0.2470, 0.6164])9 Batch time: 1.2648\n",
      "tensor([ 0.7267, -0.3421,  0.3648,  0.2696])tch time: 1.1103\n",
      "tensor([0.6545, 0.3056, 0.2220, 0.2902])9 Batch time: 1.2276\n",
      "tensor([-2.0610e-04, -3.5303e-01,  3.5526e-01,  7.2734e-01])\n",
      "tensor([ 0.7189,  0.7282, -0.0785, -0.1574])tch time: 1.2026\n",
      "tensor([0.7322, 0.4155, 0.7318, 0.3463])9 Batch time: 1.2602\n",
      "tensor([ 0.3659, -0.1197,  0.6606,  0.3649])tch time: 1.1833\n",
      "tensor([0.7296, 0.3663, 0.3493, 0.7197])39 Batch time: 1.1861\n",
      "tensor([0.3917, 0.3368, 0.3667, 0.3435])39 Batch time: 1.2181\n",
      "tensor([0.3247, 0.3328, 0.3129, 0.6218])39 Batch time: 1.2898\n",
      "tensor([-0.3868,  0.7286,  0.1302,  0.7284])atch time: 1.1856\n",
      "tensor([0.3650, 0.1465, 0.6680, 0.7027])39 Batch time: 1.1474\n",
      "tensor([0.3518, 0.7317, 0.5412, 0.3375])39 Batch time: 1.2155\n",
      "tensor([0.7317, 0.7322, 0.7320, 0.6269])39 Batch time: 1.3401\n",
      "tensor([0.3645, 0.7320, 0.6541, 0.3663])39 Batch time: 1.1560\n",
      "tensor([ 0.3625,  0.7227,  0.7323, -0.0598])atch time: 1.2066\n",
      "tensor([0.3142, 0.3563, 0.3590, 0.2240])39 Batch time: 1.1204\n",
      "tensor([ 0.6468,  0.1379, -0.3249,  0.0252])atch time: 1.1071\n",
      "tensor([0.3148, 0.3167, 0.7192, 0.6113])39 Batch time: 0.9824\n",
      "tensor([ 0.7069, -0.0380,  0.6753,  0.6935])atch time: 1.1103\n",
      "tensor([ 0.3263, -0.1981,  0.7310,  0.3579])atch time: 1.1167\n",
      "tensor([0.7276, 0.7142, 0.7171, 0.6404])39 Batch time: 0.9764\n",
      "tensor([ 0.5521, -0.1845, -0.0044,  0.3668])atch time: 1.3063\n",
      "tensor([ 0.7000,  0.2650, -0.3274,  0.3462])atch time: 1.2277\n",
      "tensor([0.7300, 0.7292, 0.3521, 0.4376])39 Batch time: 1.2077\n",
      "tensor([ 0.0976,  0.7187, -0.0375,  0.6668])atch time: 0.9922\n",
      "tensor([ 0.2284, -0.2648,  0.2419,  0.2706])atch time: 1.2145\n",
      "tensor([0.7319, 0.7167, 0.3317, 0.6621])39 Batch time: 1.1544\n",
      "tensor([0.3412, 0.3118, 0.6211, 0.3588])39 Batch time: 1.2436\n",
      "tensor([ 0.7098, -0.0797,  0.3588,  0.7051])atch time: 1.1355\n",
      "tensor([0.4333, 0.1386, 0.7307, 0.7313])39 Batch time: 1.2493\n",
      "tensor([0.2489, 0.6299, 0.2232, 0.3070])39 Batch time: 1.3448\n",
      "tensor([0.1372, 0.3375, 0.7182, 0.2309])39 Batch time: 1.2118\n",
      "tensor([0.6773, 0.0281, 0.7138, 0.7218])39 Batch time: 1.1694\n",
      "tensor([-0.3509,  0.0964, -0.3213, -0.0030])atch time: 1.1104\n",
      "Phase: validation   Epoch: 26/30 Loss: 0.3267 Acc: 0.9211        \n",
      "Phase: train Epoch: 27/30 Loss: 0.3897 Acc: 0.8811        \n",
      "tensor([0.3555, 0.7089, 0.5629, 0.0017])\n",
      "tensor([0.2953, 0.5295, 0.5691, 0.7170])9 Batch time: 1.2604\n",
      "tensor([0.3668, 0.6236, 0.3320, 0.2643])9 Batch time: 1.3410\n",
      "tensor([-0.0712,  0.2673,  0.6783,  0.5966])tch time: 1.2097\n",
      "tensor([ 0.2455,  0.7317, -0.3636,  0.5021])tch time: 1.2665\n",
      "tensor([0.1861, 0.3644, 0.4482, 0.2421])9 Batch time: 1.1719\n",
      "tensor([0.7302, 0.7300, 0.2906, 0.6396])9 Batch time: 1.1068\n",
      "tensor([0.7321, 0.6606, 0.7025, 0.4732])9 Batch time: 0.9477\n",
      "tensor([0.7301, 0.3658, 0.3567, 0.0252])9 Batch time: 0.9255\n",
      "tensor([0.3644, 0.7309, 0.2724, 0.6479])9 Batch time: 0.9048\n",
      "tensor([0.7144, 0.6700, 0.7244, 0.7228])39 Batch time: 1.0009\n",
      "tensor([0.3647, 0.3109, 0.7145, 0.0119])39 Batch time: 1.0384\n",
      "tensor([0.3310, 0.1879, 0.3450, 0.3135])39 Batch time: 1.0378\n",
      "tensor([0.6888, 0.7075, 0.1998, 0.3431])39 Batch time: 1.0404\n",
      "tensor([0.7253, 0.0018, 0.7323, 0.3664])39 Batch time: 0.9880\n",
      "tensor([ 0.7281, -0.4220,  0.7172,  0.2101])atch time: 0.9889\n",
      "tensor([ 0.7293,  0.3563,  0.7267, -0.3411])atch time: 1.0183\n",
      "tensor([0.3658, 0.3174, 0.3398, 0.3190])39 Batch time: 1.0477\n",
      "tensor([ 0.7261,  0.0767,  0.7250, -0.4031])atch time: 1.0754\n",
      "tensor([0.2683, 0.7087, 0.3070, 0.7283])39 Batch time: 0.9815\n",
      "tensor([0.1451, 0.1572, 0.7323, 0.3070])39 Batch time: 0.9456\n",
      "tensor([ 0.3447,  0.2351,  0.6790, -0.3788])atch time: 0.9205\n",
      "tensor([ 0.2781,  0.1635, -0.3608, -0.1040])atch time: 0.8830\n",
      "tensor([0.1561, 0.3371, 0.3666, 0.1363])39 Batch time: 0.8802\n",
      "tensor([0.1674, 0.3579, 0.7320, 0.7324])39 Batch time: 0.8998\n",
      "tensor([ 0.0334,  0.5504, -0.0106,  0.6757])atch time: 1.1265\n",
      "tensor([ 0.6401, -0.0246,  0.7278,  0.3413])atch time: 1.1286\n",
      "tensor([0.3484, 0.3201, 0.0983, 0.3574])39 Batch time: 1.1580\n",
      "tensor([ 0.7290,  0.3489,  0.4103, -0.2309])atch time: 1.2510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1600,  0.4992,  0.3353,  0.2935])atch time: 1.3428\n",
      "tensor([0.6969, 0.3660, 0.7312, 0.7323])39 Batch time: 1.2796\n",
      "tensor([0.7321, 0.2362, 0.7217, 0.7058])39 Batch time: 1.2062\n",
      "tensor([0.7323, 0.3529, 0.6926, 0.7023])39 Batch time: 1.1474\n",
      "tensor([ 0.6220, -0.4845,  0.4292,  0.7306])atch time: 1.1890\n",
      "tensor([ 0.7285,  0.3505, -0.4043, -0.1218])atch time: 1.0821\n",
      "tensor([0.3653, 0.2585, 0.3537, 0.6967])39 Batch time: 1.3043\n",
      "tensor([0.0537, 0.7321, 0.6340, 0.2495])39 Batch time: 1.1283\n",
      "tensor([ 0.1349, -0.0644,  0.7324,  0.7023])atch time: 1.1555\n",
      "Phase: validation   Epoch: 27/30 Loss: 0.3288 Acc: 0.9276        \n",
      "Phase: train Epoch: 28/30 Loss: 0.3645 Acc: 0.9098        \n",
      "tensor([ 0.3633,  0.5738, -0.3165,  0.2530])\n",
      "tensor([0.7080, 0.2818, 0.6980, 0.3664])9 Batch time: 1.0922\n",
      "tensor([0.1341, 0.7190, 0.3297, 0.1846])9 Batch time: 1.1324\n",
      "tensor([ 0.6402,  0.7283, -0.5032, -0.1683])tch time: 1.2961\n",
      "tensor([0.3644, 0.6952, 0.3488, 0.3572])9 Batch time: 1.1834\n",
      "tensor([0.3627, 0.3438, 0.1708, 0.7316])9 Batch time: 1.2630\n",
      "tensor([0.2288, 0.7290, 0.3480, 0.7132])9 Batch time: 1.1937\n",
      "tensor([0.3384, 0.3003, 0.3669, 0.3611])9 Batch time: 1.1179\n",
      "tensor([ 0.1344,  0.3535,  0.3226, -0.2018])tch time: 1.1001\n",
      "tensor([0.3563, 0.3408, 0.3669, 0.6947])9 Batch time: 1.0706\n",
      "tensor([0.3993, 0.1421, 0.4630, 0.0219])39 Batch time: 1.1452\n",
      "tensor([ 0.1776,  0.7313,  0.6652, -0.3349])atch time: 1.1949\n",
      "tensor([0.7313, 0.7263, 0.3593, 0.7320])39 Batch time: 1.2671\n",
      "tensor([0.7324, 0.7074, 0.7185, 0.3663])39 Batch time: 1.2752\n",
      "tensor([-0.1226,  0.7204,  0.7321,  0.3006])atch time: 1.2441\n",
      "tensor([0.3667, 0.7201, 0.6369, 0.6662])39 Batch time: 1.1638\n",
      "tensor([0.2402, 0.6935, 0.6072, 0.3429])39 Batch time: 1.3214\n",
      "tensor([0.3659, 0.3661, 0.4834, 0.3512])39 Batch time: 1.1870\n",
      "tensor([0.3618, 0.7256, 0.1481, 0.3621])39 Batch time: 1.1996\n",
      "tensor([ 0.7319,  0.7082,  0.7016, -0.2767])atch time: 1.2103\n",
      "tensor([ 0.5719,  0.2457, -0.4479,  0.2874])atch time: 1.1365\n",
      "tensor([0.2649, 0.3114, 0.3616, 0.6018])39 Batch time: 1.1506\n",
      "tensor([ 0.3457,  0.7322,  0.3505, -0.0181])atch time: 1.1638\n",
      "tensor([0.7222, 0.7216, 0.3628, 0.2802])39 Batch time: 1.1761\n",
      "tensor([ 0.7286,  0.6741, -0.5048,  0.6102])atch time: 1.1715\n",
      "tensor([0.7315, 0.7263, 0.2242, 0.7271])39 Batch time: 1.2441\n",
      "tensor([0.2855, 0.2672, 0.5950, 0.7089])39 Batch time: 1.2016\n",
      "tensor([ 0.3238,  0.0524, -0.1912, -0.3223])atch time: 1.2235\n",
      "tensor([0.4606, 0.7317, 0.3663, 0.3250])39 Batch time: 1.2989\n",
      "tensor([0.3095, 0.7323, 0.3515, 0.6513])39 Batch time: 1.5503\n",
      "tensor([ 0.3511, -0.3486,  0.0683,  0.3293])atch time: 1.2026\n",
      "tensor([0.7224, 0.2544, 0.2735, 0.3632])39 Batch time: 1.2531\n",
      "tensor([-0.0809, -0.1479,  0.3099,  0.3665])atch time: 1.2953\n",
      "tensor([0.7110, 0.6949, 0.4042, 0.3604])39 Batch time: 1.3761\n",
      "tensor([ 0.4412,  0.6822, -0.3043,  0.1808])atch time: 1.2751\n",
      "tensor([0.2165, 0.3588, 0.5887, 0.3308])39 Batch time: 1.2386\n",
      "tensor([0.5896, 0.3661, 0.4830, 0.7301])39 Batch time: 1.1804\n",
      "tensor([0.5656, 0.5980, 0.6862, 0.3568])39 Batch time: 1.2184\n",
      "Phase: validation   Epoch: 28/30 Loss: 0.3372 Acc: 0.9079        \n",
      "Phase: train Epoch: 29/30 Loss: 0.4359 Acc: 0.8484        \n",
      "tensor([ 0.7321,  0.1756,  0.1974, -0.5383])\n",
      "tensor([0.6753, 0.3406, 0.6972, 0.6671])9 Batch time: 1.0683\n",
      "tensor([ 0.2867,  0.3668,  0.2348, -0.1760])tch time: 1.1269\n",
      "tensor([0.1570, 0.6515, 0.4943, 0.3652])9 Batch time: 1.1431\n",
      "tensor([0.7257, 0.7324, 0.7310, 0.3652])9 Batch time: 1.1772\n",
      "tensor([0.2968, 0.3576, 0.7315, 0.5257])9 Batch time: 1.1813\n",
      "tensor([ 0.7322, -0.2495,  0.2912,  0.1969])tch time: 1.1573\n",
      "tensor([0.6503, 0.6149, 0.2712, 0.7086])9 Batch time: 1.1428\n",
      "tensor([ 0.3502,  0.1890,  0.2935, -0.3552])tch time: 1.1341\n",
      "tensor([ 0.3667, -0.1298,  0.3493,  0.3281])tch time: 1.1118\n",
      "tensor([ 0.6388, -0.3085,  0.3489,  0.6988])atch time: 1.0938\n",
      "tensor([0.2401, 0.3373, 0.7323, 0.1100])39 Batch time: 1.0816\n",
      "tensor([0.6595, 0.2927, 0.1965, 0.3661])39 Batch time: 1.0946\n",
      "tensor([0.3593, 0.7265, 0.6979, 0.7320])39 Batch time: 1.0527\n",
      "tensor([0.7096, 0.3065, 0.7137, 0.7316])39 Batch time: 1.0787\n",
      "tensor([0.0323, 0.6959, 0.3648, 0.3619])39 Batch time: 1.1413\n",
      "tensor([ 0.6997, -0.5537,  0.6128,  0.3663])atch time: 1.1917\n",
      "tensor([0.3367, 0.0076, 0.6841, 0.7301])39 Batch time: 1.1116\n",
      "tensor([0.6357, 0.2308, 0.6233, 0.6093])39 Batch time: 1.1680\n",
      "tensor([ 0.7323, -0.3826, -0.0150, -0.3914])atch time: 1.1559\n",
      "tensor([0.0155, 0.6206, 0.3607, 0.2983])39 Batch time: 1.1487\n",
      "tensor([0.7323, 0.7289, 0.0032, 0.7309])39 Batch time: 1.1258\n",
      "tensor([0.5499, 0.7288, 0.7316, 0.1147])39 Batch time: 1.0252\n",
      "tensor([0.2623, 0.3574, 0.3628, 0.3544])39 Batch time: 1.0109\n",
      "tensor([0.2332, 0.6238, 0.3382, 0.7299])39 Batch time: 1.0234\n",
      "tensor([0.7307, 0.0831, 0.7175, 0.7235])39 Batch time: 1.0050\n",
      "tensor([0.3675, 0.2779, 0.3409, 0.2876])39 Batch time: 1.0266\n",
      "tensor([ 0.2668, -0.2480,  0.0314,  0.3665])atch time: 0.9889\n",
      "tensor([ 0.3581,  0.7086, -0.1551,  0.6948])atch time: 0.9778\n",
      "tensor([-0.0129,  0.6656,  0.6930,  0.2570])atch time: 0.9805\n",
      "tensor([0.3641, 0.3141, 0.3376, 0.3625])39 Batch time: 0.9971\n",
      "tensor([ 0.1453, -0.1453,  0.7286,  0.3602])atch time: 1.0015\n",
      "tensor([ 0.5977,  0.7256,  0.3319, -0.0759])atch time: 1.0424\n",
      "tensor([ 0.2887,  0.7126,  0.3505, -0.3064])atch time: 1.0384\n",
      "tensor([0.3655, 0.4189, 0.7272, 0.1438])39 Batch time: 1.0229\n",
      "tensor([0.7101, 0.6055, 0.3384, 0.3513])39 Batch time: 1.0123\n",
      "tensor([0.3289, 0.7129, 0.3497, 0.5081])39 Batch time: 0.9949\n",
      "tensor([0.4862, 0.3606, 0.3522, 0.7214])39 Batch time: 0.9629\n",
      "Phase: validation   Epoch: 29/30 Loss: 0.3363 Acc: 0.9145        \n",
      "Phase: train Epoch: 30/30 Loss: 0.3892 Acc: 0.8730        \n",
      "tensor([ 0.6866,  0.7306,  0.5910, -0.3261])\n",
      "tensor([0.2155, 0.6997, 0.2195, 0.7319])9 Batch time: 0.7294\n",
      "tensor([ 0.3667,  0.0420,  0.7320, -0.4227])tch time: 0.8218\n",
      "tensor([0.2367, 0.3385, 0.3407, 0.7013])9 Batch time: 0.9649\n",
      "tensor([0.7152, 0.3529, 0.5456, 0.7306])9 Batch time: 0.9403\n",
      "tensor([0.7321, 0.2373, 0.3656, 0.7309])9 Batch time: 0.9479\n",
      "tensor([0.2348, 0.7164, 0.7316, 0.3557])9 Batch time: 0.9221\n",
      "tensor([0.0115, 0.7259, 0.1824, 0.7220])9 Batch time: 0.9959\n",
      "tensor([0.7237, 0.7323, 0.7290, 0.0543])9 Batch time: 0.9503\n",
      "tensor([0.7323, 0.3611, 0.0431, 0.2866])9 Batch time: 0.9210\n",
      "tensor([0.7304, 0.2739, 0.3384, 0.7323])39 Batch time: 0.9126\n",
      "tensor([ 0.2231,  0.2942, -0.0482, -0.4458])atch time: 1.0175\n",
      "tensor([0.5777, 0.3404, 0.7302, 0.7316])39 Batch time: 1.0090\n",
      "tensor([ 0.3430,  0.3035,  0.3076, -0.0068])atch time: 1.0055\n",
      "tensor([-0.1599,  0.7141,  0.7322,  0.7324])atch time: 0.9793\n",
      "tensor([0.6939, 0.1798, 0.7240, 0.6665])39 Batch time: 0.9664\n",
      "tensor([0.7322, 0.3300, 0.7322, 0.3662])39 Batch time: 0.9390\n",
      "tensor([ 0.1502,  0.7079,  0.6925, -0.0505])atch time: 0.9190\n",
      "tensor([0.7084, 0.3184, 0.1903, 0.6809])39 Batch time: 0.9810\n",
      "tensor([-0.0058,  0.7323,  0.7176,  0.7121])atch time: 0.9683\n",
      "tensor([ 0.1446, -0.3074,  0.6430, -0.2298])atch time: 0.9698\n",
      "tensor([0.7272, 0.0634, 0.3467, 0.4165])39 Batch time: 0.9650\n",
      "tensor([0.1511, 0.7237, 0.2439, 0.1242])39 Batch time: 0.9689\n",
      "tensor([0.3834, 0.3597, 0.7029, 0.1469])39 Batch time: 1.0054\n",
      "tensor([ 0.6809,  0.1700,  0.3182, -0.3376])atch time: 1.0005\n",
      "tensor([ 0.7301,  0.7228, -0.0172,  0.2754])atch time: 1.0765\n",
      "tensor([ 0.0421, -0.0860,  0.7309,  0.3458])atch time: 1.0272\n",
      "tensor([0.5971, 0.7283, 0.3328, 0.3406])39 Batch time: 1.0141\n",
      "tensor([0.6828, 0.6955, 0.3310, 0.3654])39 Batch time: 1.0332\n",
      "tensor([ 0.1998,  0.2926,  0.5375, -0.0647])atch time: 1.0705\n",
      "tensor([-0.3204,  0.1195, -0.3138,  0.6980])atch time: 1.0308\n",
      "tensor([0.7304, 0.3650, 0.7253, 0.2345])39 Batch time: 1.0531\n",
      "tensor([0.3105, 0.0089, 0.4909, 0.5796])39 Batch time: 1.1063\n",
      "tensor([0.7322, 0.3578, 0.6980, 0.7324])39 Batch time: 1.0006\n",
      "tensor([0.3625, 0.7262, 0.3414, 0.3033])39 Batch time: 0.9523\n",
      "tensor([-0.2283, -0.2940,  0.6572,  0.3632])atch time: 1.0422\n",
      "tensor([0.5215, 0.3427, 0.2397, 0.3368])39 Batch time: 0.9614\n",
      "tensor([ 0.1097,  0.6588, -0.3441,  0.7312])atch time: 1.0014\n",
      "Phase: validation   Epoch: 30/30 Loss: 0.3212 Acc: 0.9539        \n",
      "Training completed in 218m 21s\n",
      "Best test loss: 0.3163 | Best test accuracy: 0.9539\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Training our model\n",
    "'''\n",
    "model_hybrid, p = train_model(model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model_hybrid.eval()\n",
    "#real label\n",
    "y = []\n",
    "#scores. Calculate our pred using this\n",
    "scores = []\n",
    "for i, (inputs, labels) in enumerate(dataloaders['validation']):\n",
    "    l.append(labels.tolist())\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_hybrid(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    sm = torch.nn.Softmax()\n",
    "    probabilities = sm(outputs) \n",
    "    \n",
    "    for j in range(len(labels.tolist())):\n",
    "        y.append(labels.tolist()[j])\n",
    "        scores.append(probabilities.tolist()[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    if scores[i] >= 0.5:\n",
    "        scores[i] = 1\n",
    "    else:\n",
    "        scores[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axarr = plt.subplots(1,1, figsize=(15, 8),constrained_layout=True)\n",
    "fig.suptitle(\"ROC Curves\", fontsize=18)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y, scores)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "axarr.set_title('ROC - My MLP', fontsize=12)\n",
    "axarr.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "axarr.legend(loc = 'right', fontsize=20)\n",
    "axarr.plot([0, 1], [0, 1],'r--')\n",
    "axarr.set_xlim([0, 1])\n",
    "axarr.set_ylim([0, 1])\n",
    "axarr.set_ylabel('True Positive Rate', fontsize=12)\n",
    "axarr.set_xlabel('False Positive Rate', fontsize=12)\n",
    "plt.savefig('C://Users//Vishaal//Documents//GitHub//Hybrid-Transfer-Learning//Plots//Quantum_ROC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
